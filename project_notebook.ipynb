{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE9ShpcD-MA4"
      },
      "source": [
        "## deduplicate rbp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV2YmtWCVkYi",
        "outputId": "31581864-82c7-486e-fc74-43a929881b7a"
      },
      "outputs": [],
      "source": [
        "# %% RBP-only dedup (drop-in replacement for old script outputs)\n",
        "# Produces exactly the same files/format as the original:\n",
        "#   dedup_pwm_out/\n",
        "#     - training_keep_indices.txt\n",
        "#     - training_drop_indices.txt\n",
        "#     - training_map.csv                  (orig_col,cluster_id,representative_col)\n",
        "#     - training_duplicate_clusters_pwm.json\n",
        "#\n",
        "# No RNA dedup. RNAs are only used to build PWMs for duplicate-RBP decisions.\n",
        "\n",
        "import os, json, csv\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------- paths -----------------------------\n",
        "RBP_FILE    = \"training_RBPs2.txt\"    # 200 protein sequences (1 per line)\n",
        "SEQ_FILE    = \"training_seqs.txt\"     # 120,678 RNA sequences (T/U -> U)\n",
        "MATRIX_FILE = \"training_data2.txt\"    # 120,678 x 200 float matrix (whitespace)\n",
        "OUT_DIR     = \"dedup_pwm_out\"\n",
        "\n",
        "# PWM construction from probe k-mers\n",
        "KMER_K        = 6\n",
        "PWM_SOFTMAX_B = 4.0\n",
        "PWM_PSEUDO    = 1e-3\n",
        "\n",
        "# Selection rule: \"consensus\" (paper-style) or \"ic\" (old behavior)\n",
        "REP_SELECTION = \"consensus\"\n",
        "\n",
        "# ----------------------------- helpers -----------------------------\n",
        "def read_lines(path, normalize_rna=False):\n",
        "    seqs = []\n",
        "    with open(path, 'r') as f:\n",
        "        for ln in f:\n",
        "            s = ln.strip().upper()\n",
        "            if not s:\n",
        "                continue\n",
        "            if normalize_rna:\n",
        "                s = s.replace('T','U')\n",
        "            seqs.append(s)\n",
        "    return seqs\n",
        "\n",
        "def find_duplicate_clusters(seq_list):\n",
        "    idx_of = {}\n",
        "    clusters = []\n",
        "    for i, s in enumerate(seq_list):\n",
        "        if s in idx_of:\n",
        "            clusters[idx_of[s]].append(i)\n",
        "        else:\n",
        "            idx_of[s] = len(clusters)\n",
        "            clusters.append([i])\n",
        "    return [c for c in clusters if len(c) > 1]\n",
        "\n",
        "NUC2I = {'A':0,'C':1,'G':2,'U':3}\n",
        "\n",
        "def encode_kmers_of_seq(seq, k=6):\n",
        "    arr = np.array([NUC2I.get(ch, -1) for ch in seq], dtype=np.int64)\n",
        "    if (arr < 0).any():\n",
        "        return np.empty(0, dtype=np.int32)\n",
        "    n = len(arr); m = n - k + 1\n",
        "    if m <= 0:\n",
        "        return np.empty(0, dtype=np.int32)\n",
        "    codes = np.empty(m, dtype=np.int32)\n",
        "    base = 0\n",
        "    pow4 = 4**(k-1)\n",
        "    for i in range(k):\n",
        "        base = base*4 + int(arr[i])\n",
        "    codes[0] = base\n",
        "    for i in range(1, m):\n",
        "        base = (base - int(arr[i-1])*pow4)*4 + int(arr[i+k-1])\n",
        "        codes[i] = base\n",
        "    return codes\n",
        "\n",
        "def precompute_probe_kmers(rna_list, k=6):\n",
        "    offsets = [0]\n",
        "    chunks = []\n",
        "    for s in rna_list:\n",
        "        c = encode_kmers_of_seq(s, k)\n",
        "        chunks.append(c)\n",
        "        offsets.append(offsets[-1] + len(c))\n",
        "    flat = np.concatenate(chunks) if chunks else np.empty(0, dtype=np.int32)\n",
        "    offsets = np.array(offsets, dtype=np.int64)\n",
        "    per_probe_counts = offsets[1:] - offsets[:-1]\n",
        "    counts_by_code = np.bincount(flat, minlength=4**k)\n",
        "    return flat, offsets, per_probe_counts, counts_by_code\n",
        "\n",
        "def read_matrix_columns_txt(path, usecols):\n",
        "    \"\"\"Return (N_rows x len(cols)) float32 array AND the sorted col list used.\"\"\"\n",
        "    cols_sorted = sorted(usecols)\n",
        "    data_cols = [[] for _ in cols_sorted]\n",
        "    with open(path, 'r') as f:\n",
        "        for ln in f:\n",
        "            parts = ln.strip().split()\n",
        "            if not parts:\n",
        "                continue\n",
        "            for j, c in enumerate(cols_sorted):\n",
        "                data_cols[j].append(float(parts[c]))\n",
        "    M = np.stack([np.array(col, dtype=np.float32) for col in data_cols], axis=1)\n",
        "    return M, cols_sorted\n",
        "\n",
        "def pwm_from_kmer_scores(avg_scores, counts_by_code, k=6, beta=4.0, pseudocount=1e-3):\n",
        "    V = 4**k\n",
        "    assert avg_scores.shape[0] == V\n",
        "    valid = counts_by_code > 0\n",
        "    codes = np.nonzero(valid)[0]\n",
        "    weights = counts_by_code[valid].astype(np.float64)\n",
        "    vals = avg_scores[valid].astype(np.float64)\n",
        "\n",
        "    pwm_scores = np.zeros((k,4), dtype=np.float64)\n",
        "    denom = np.zeros((k,4), dtype=np.float64)\n",
        "    pow4 = np.array([4**(k-1-p) for p in range(k)], dtype=np.int64)\n",
        "    for pos in range(k):\n",
        "        base = (codes // pow4[pos]) % 4\n",
        "        for b in range(4):\n",
        "            mask = (base == b)\n",
        "            if mask.any():\n",
        "                w = weights[mask]\n",
        "                v = vals[mask]\n",
        "                pwm_scores[pos, b] = (w * v).sum()\n",
        "                denom[pos, b] = w.sum()\n",
        "    mean = np.divide(pwm_scores, np.maximum(denom, 1e-12))\n",
        "    pwm = np.zeros_like(mean)\n",
        "    for pos in range(k):\n",
        "        x = mean[pos] - mean[pos].mean()\n",
        "        x = np.exp(beta * x) + pseudocount\n",
        "        pwm[pos] = x / x.sum()\n",
        "    return pwm\n",
        "\n",
        "def info_content_bits(pwm, bg=0.25, eps=1e-9):\n",
        "    P = pwm + eps\n",
        "    P = P / P.sum(axis=1, keepdims=True)\n",
        "    return float(np.sum(P * (np.log2(P) - np.log2(bg))))\n",
        "\n",
        "# --------- NEW: paper-style PWM alignment metrics (max-corr / min-sKL) ----------\n",
        "def _overlap_slices(L1, L2, shift):\n",
        "    # shift>0: Q is moved right; compare P[p] with Q[p-shift]\n",
        "    start_p = max(0, shift)\n",
        "    end_p   = min(L1, L2 + shift)\n",
        "    if end_p - start_p <= 0: return None\n",
        "    start_q = start_p - shift\n",
        "    end_q   = start_q + (end_p - start_p)\n",
        "    return slice(start_p, end_p), slice(start_q, end_q)\n",
        "\n",
        "def pwm_maxcorr(P, Q):\n",
        "    \"\"\"Maximum column-wise Pearson (flattened over bases) across all shifts.\"\"\"\n",
        "    L1, L2 = P.shape[0], Q.shape[0]\n",
        "    best = 0.0\n",
        "    for s in range(-(L2-1), L1):\n",
        "        sl = _overlap_slices(L1, L2, s)\n",
        "        if sl is None: continue\n",
        "        p = P[sl[0]].reshape(-1)\n",
        "        q = Q[sl[1]].reshape(-1)\n",
        "        mp, mq = p.mean(), q.mean()\n",
        "        sp, sq = p.std(),  q.std()\n",
        "        if sp == 0 or sq == 0: continue\n",
        "        r = float(np.dot(p-mp, q-mq) / (len(p)*sp*sq))\n",
        "        if r > best: best = r\n",
        "    return best\n",
        "\n",
        "def pwm_min_sKL(P, Q, eps=1e-12):\n",
        "    \"\"\"Minimum symmetric KL across all shifts (reporting metric).\"\"\"\n",
        "    L1, L2 = P.shape[0], Q.shape[0]\n",
        "    best = float(\"inf\")\n",
        "    for s in range(-(L2-1), L1):\n",
        "        sl = _overlap_slices(L1, L2, s)\n",
        "        if sl is None: continue\n",
        "        p = P[sl[0]] + eps\n",
        "        q = Q[sl[1]] + eps\n",
        "        p = p / p.sum(axis=1, keepdims=True)\n",
        "        q = q / q.sum(axis=1, keepdims=True)\n",
        "        d = float(0.5 * (np.sum(p * (np.log(p) - np.log(q))) +\n",
        "                         np.sum(q * (np.log(q) - np.log(p)))))\n",
        "        if d < best: best = d\n",
        "    return best if np.isfinite(best) else 0.0\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# ----------------------------- main flow -----------------------------\n",
        "def main():\n",
        "    assert os.path.exists(RBP_FILE),    f\"Missing {RBP_FILE}\"\n",
        "    assert os.path.exists(SEQ_FILE),    f\"Missing {SEQ_FILE}\"\n",
        "    assert os.path.exists(MATRIX_FILE), f\"Missing {MATRIX_FILE}\"\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "    print(\"[1] Loading proteins & finding duplicate RBPs…\")\n",
        "    train_rbps = read_lines(RBP_FILE, normalize_rna=False)\n",
        "    dup_clusters = find_duplicate_clusters(train_rbps)\n",
        "    if not dup_clusters:\n",
        "        print(\"   No duplicate protein sequences found — nothing to deduplicate.\")\n",
        "        keep_idx = list(range(len(train_rbps)))\n",
        "        with open(os.path.join(OUT_DIR, \"training_keep_indices.txt\"), \"w\") as f:\n",
        "            for i in keep_idx: f.write(f\"{i}\\n\")\n",
        "        with open(os.path.join(OUT_DIR, \"training_map.csv\"), \"w\", newline=\"\") as f:\n",
        "            w = csv.writer(f); w.writerow([\"orig_col\",\"cluster_id\",\"representative_col\"])\n",
        "            for i in keep_idx: w.writerow([i, -1, i])\n",
        "        # mimic old behavior: no drop file / cluster json when no dups\n",
        "        print(f\"   Wrote outputs to {OUT_DIR}/ ; exiting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"   Found {len(dup_clusters)} duplicate groups (covering {sum(len(c) for c in dup_clusters)} columns).\")\n",
        "\n",
        "    print(\"[2] Precomputing k-mers over all RNA probes (k=%d)…\" % KMER_K)\n",
        "    rna_probes = read_lines(SEQ_FILE, normalize_rna=True)\n",
        "    codes_flat, offsets, per_probe_counts, counts_by_code = precompute_probe_kmers(rna_probes, k=KMER_K)\n",
        "    N = len(rna_probes)\n",
        "    repeat_idx = np.repeat(np.arange(N, dtype=np.int32), per_probe_counts)\n",
        "    print(f\"   Probes: {N:,}, total {codes_flat.size:,} {KMER_K}-mers.\")\n",
        "\n",
        "    # gather all duplicate columns\n",
        "    all_dup_cols = sorted({i for grp in dup_clusters for i in grp})\n",
        "    print(f\"[3] Reading intensities for {len(all_dup_cols)} duplicate columns…\")\n",
        "    Y, cols_sorted = read_matrix_columns_txt(MATRIX_FILE, all_dup_cols)  # (N, K_dup)\n",
        "    col_to_pos = {c: j for j, c in enumerate(cols_sorted)}\n",
        "\n",
        "    print(\"[4] Building PWMs & selecting representatives (paper-style consensus)…\")\n",
        "    cluster_reports = []\n",
        "    rep_set = set()\n",
        "\n",
        "    for cid, grp in enumerate(dup_clusters):\n",
        "        col_info = []\n",
        "        pwms = {}\n",
        "        # build PWM for each member\n",
        "        for col in grp:\n",
        "            y = Y[:, col_to_pos[col]].astype(np.float32)   # (N,)\n",
        "            y_rep = y[repeat_idx]                          # expand per-probe to per-kmer weights\n",
        "            sums = np.bincount(codes_flat, weights=y_rep, minlength=4**KMER_K)\n",
        "            avg  = sums / np.maximum(counts_by_code, 1)\n",
        "            pwm  = pwm_from_kmer_scores(avg, counts_by_code, k=KMER_K,\n",
        "                                        beta=PWM_SOFTMAX_B, pseudocount=PWM_PSEUDO)\n",
        "            ic   = info_content_bits(pwm)\n",
        "            col_info.append({\"col\": int(col), \"IC_bits\": float(ic)})\n",
        "            pwms[int(col)] = pwm\n",
        "\n",
        "        # pairwise metrics (paper-style)\n",
        "        pairwise = []\n",
        "        for i in range(len(grp)):\n",
        "            for j in range(i+1, len(grp)):\n",
        "                ci, cj = int(grp[i]), int(grp[j])\n",
        "                r  = pwm_maxcorr(pwms[ci], pwms[cj])   # ADDED\n",
        "                d  = pwm_min_sKL(pwms[ci], pwms[cj])   # ADDED\n",
        "                pairwise.append({\n",
        "                    \"col_i\": ci, \"col_j\": cj,\n",
        "                    \"maxcorr\": float(r),\n",
        "                    \"min_sKL\": float(d)\n",
        "                })\n",
        "\n",
        "        # consensus score per member = mean of pairwise maxcorr to others\n",
        "        if REP_SELECTION.lower() == \"consensus\" and len(grp) > 1:\n",
        "            corr_map = {}\n",
        "            for p in pairwise:\n",
        "                corr_map.setdefault(p[\"col_i\"], []).append(p[\"maxcorr\"])\n",
        "                corr_map.setdefault(p[\"col_j\"], []).append(p[\"maxcorr\"])\n",
        "            for rec in col_info:\n",
        "                rec[\"consensus_corr_mean\"] = float(np.mean(corr_map.get(rec[\"col\"], [1.0])))\n",
        "            # choose by consensus, tie-break by IC\n",
        "            col_info.sort(key=lambda d: (d.get(\"consensus_corr_mean\", 0.0), d[\"IC_bits\"]), reverse=True)\n",
        "        else:\n",
        "            # old behavior (IC only)\n",
        "            col_info.sort(key=lambda d: d[\"IC_bits\"], reverse=True)\n",
        "\n",
        "        rep_col = col_info[0][\"col\"]\n",
        "        rep_set.add(rep_col)\n",
        "\n",
        "        cluster_reports.append({\n",
        "            \"cluster_id\": cid,\n",
        "            \"members\": [int(c) for c in grp],\n",
        "            \"representative_col\": int(rep_col),\n",
        "            \"members_sorted\": col_info,        # includes IC and (if used) consensus_corr_mean\n",
        "            \"pairwise_pwm_stats\": pairwise     # includes maxcorr and min_sKL (paper-style)\n",
        "        })\n",
        "\n",
        "    # keep = non-duplicates + representatives\n",
        "    all_cols_set = set(range(len(train_rbps)))\n",
        "    dup_cols_set = set(all_dup_cols)\n",
        "    nondup_cols  = sorted(all_cols_set - dup_cols_set)\n",
        "    keep_cols    = sorted(nondup_cols + list(rep_set))\n",
        "    drop_cols    = sorted(dup_cols_set - rep_set)\n",
        "\n",
        "    print(\"\\n[5] Writing EXACT old-style outputs to dedup_pwm_out/ …\")\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "    # keep\n",
        "    with open(os.path.join(OUT_DIR, \"training_keep_indices.txt\"), \"w\") as f:\n",
        "        for c in keep_cols: f.write(f\"{c}\\n\")\n",
        "\n",
        "    # drop\n",
        "    with open(os.path.join(OUT_DIR, \"training_drop_indices.txt\"), \"w\") as f:\n",
        "        for c in drop_cols: f.write(f\"{c}\\n\")\n",
        "\n",
        "    # map\n",
        "    with open(os.path.join(OUT_DIR, \"training_map.csv\"), \"w\", newline=\"\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"orig_col\",\"cluster_id\",\"representative_col\"])\n",
        "        for c in nondup_cols:\n",
        "            w.writerow([c, -1, c])\n",
        "        for cid, rep in enumerate(cluster_reports):\n",
        "            repc = rep[\"representative_col\"]\n",
        "            for c in rep[\"members\"]:\n",
        "                w.writerow([c, cid, repc])\n",
        "\n",
        "    # json report (same filename/key schema as old; now richer payload is fine)\n",
        "    with open(os.path.join(OUT_DIR, \"training_duplicate_clusters_pwm.json\"), \"w\") as f:\n",
        "        json.dump({\n",
        "            \"k_mer\": KMER_K,\n",
        "            \"note\": (\"Representative chosen by cluster-consensus (mean aligned Pearson) \"\n",
        "                     \"with IC tie-break; JSON also reports aligned maxcorr and min sKL.\"),\n",
        "            \"selection\": REP_SELECTION,\n",
        "            \"clusters\": cluster_reports\n",
        "        }, f, indent=2)\n",
        "\n",
        "    print(\"[Done]\")\n",
        "    print(f\" - keep indices: {OUT_DIR}/training_keep_indices.txt  (len={len(keep_cols)})\")\n",
        "    print(f\" - drop indices: {OUT_DIR}/training_drop_indices.txt  (len={len(drop_cols)})\")\n",
        "    print(f\" - mapping csv : {OUT_DIR}/training_map.csv\")\n",
        "    print(f\" - report json : {OUT_DIR}/training_duplicate_clusters_pwm.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUUfOKfjPIpg"
      },
      "source": [
        "## vienna rna + tools download  , skip if you dont want to calculate secondary struct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-H6ftsDq---"
      },
      "outputs": [],
      "source": [
        "!pip install ViennaRNA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hH_AjMZANlY"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install vienna-rna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoukTXTd-gHZ"
      },
      "source": [
        "## code to calculate secondary struct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tloJqscvvL1",
        "outputId": "14482a9c-582f-470b-8337-2f5759278290"
      },
      "outputs": [],
      "source": [
        "%%writefile rna_struct_features_jupyter.py\n",
        "# (paste the whole Jupyter-friendly code I gave you)\n",
        "\n",
        "# Generate per-sequence NPZ files with PHIME [L,5] (and optional extras) using ViennaRNA.\n",
        "# Designed to be imported or run inside a notebook.\n",
        "\n",
        "import os, re, glob, time, tempfile, shutil, subprocess, sys, json\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# --------------------- small utils ---------------------\n",
        "\n",
        "_DOT_RE = re.compile(r'^[().]+')\n",
        "\n",
        "def check_vienna(verbose: bool = True) -> bool:\n",
        "    need = [\"RNAfold\",\"RNAsubopt\",\"RNAplfold\"]\n",
        "    missing=[]\n",
        "    for name in need:\n",
        "        try:\n",
        "            subprocess.run([name, \"-h\"], capture_output=True, check=False)\n",
        "        except Exception:\n",
        "            missing.append(name)\n",
        "    if missing and verbose:\n",
        "        print(\"❌ Missing:\", \", \".join(missing), file=sys.stderr)\n",
        "        print(\"   Install on Debian/Ubuntu:\", file=sys.stderr)\n",
        "        print(\"   sudo apt-get -qq update && sudo apt-get install -y vienna-rna\", file=sys.stderr)\n",
        "    return len(missing) == 0\n",
        "\n",
        "def _run(cmd, inp: Optional[str] = None, cwd: Optional[str] = None, env: Optional[Dict[str,str]] = None):\n",
        "    env2 = os.environ.copy()\n",
        "    env2.update(env or {})\n",
        "    # Determinism + no thread oversubscription\n",
        "    env2.setdefault(\"VRNA_RANDSEED\", \"42\")\n",
        "    env2.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
        "    env2.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
        "    env2.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
        "    return subprocess.run(cmd, input=inp, text=True, capture_output=True, check=False, cwd=cwd, env=env2)\n",
        "\n",
        "def _to_rna(seq: str) -> str:\n",
        "    return seq.strip().upper().replace(\"T\",\"U\")\n",
        "\n",
        "def _parse_structs(stdout: str, L: int) -> List[str]:\n",
        "    out=[]\n",
        "    for ln in stdout.splitlines():\n",
        "        m=_DOT_RE.match(ln.strip())\n",
        "        if m:\n",
        "            s=m.group(0)\n",
        "            if len(s)==L: out.append(s)\n",
        "    return out\n",
        "\n",
        "def _dot_from_any_output(text: str, L: int) -> str:\n",
        "    best=\"\"\n",
        "    for ln in text.splitlines():\n",
        "        m=_DOT_RE.match(ln.strip())\n",
        "        if m:\n",
        "            s=m.group(0)\n",
        "            if len(s)==L and len(s)>len(best): best=s\n",
        "    return best\n",
        "\n",
        "# ----------- MFE & dot-bracket -> one-hot PHIME -----------\n",
        "\n",
        "def rnafold_mfe(sequence: str, temp_c: float = 37.0) -> Tuple[str, float]:\n",
        "    seq=_to_rna(sequence)\n",
        "    cp=_run(['RNAfold','--noPS','-T',f'{temp_c:.2f}'], f\">seq\\n{seq}\\n\")\n",
        "    db=_dot_from_any_output(cp.stdout, len(seq))\n",
        "    m = re.search(r'[-+]?\\d+(?:\\.\\d+)?', cp.stdout)\n",
        "    mfe=float(m.group(0)) if m else 0.0\n",
        "    return db, mfe\n",
        "\n",
        "def _pair_table(db: str) -> List[int]:\n",
        "    L=len(db); pt=[0]*(L+1); st=[]\n",
        "    for i,ch in enumerate(db,1):\n",
        "        if ch=='(':\n",
        "            st.append(i)\n",
        "        elif ch==')' and st:\n",
        "            j=st.pop(); pt[i]=j; pt[j]=i\n",
        "    return pt\n",
        "\n",
        "def _kids(pt, i, j):\n",
        "    kids=[]; p=i+1\n",
        "    while p<j:\n",
        "        q=pt[p]\n",
        "        if q==0: p+=1\n",
        "        elif i<p<q<j: kids.append((p,q)); p=q+1\n",
        "        else: p+=1\n",
        "    return kids\n",
        "\n",
        "def _anno_pair(pt,i,j,H,I,M):\n",
        "    ks=_kids(pt,i,j)\n",
        "    if len(ks)==0:\n",
        "        for t in range(i+1,j):\n",
        "            if pt[t]==0: H[t-1]+=1\n",
        "        return\n",
        "    if len(ks)==1:\n",
        "        k,l=ks[0]\n",
        "        for t in range(i+1,k):\n",
        "            if pt[t]==0: I[t-1]+=1\n",
        "        for t in range(l+1,j):\n",
        "            if pt[t]==0: I[t-1]+=1\n",
        "        _anno_pair(pt,k,l,H,I,M); return\n",
        "    prev=i\n",
        "    for (k,l) in ks:\n",
        "        for t in range(prev+1,k):\n",
        "            if pt[t]==0: M[t-1]+=1\n",
        "        _anno_pair(pt,k,l,H,I,M); prev=l\n",
        "    for t in range(prev+1,j):\n",
        "        if pt[t]==0: M[t-1]+=1\n",
        "\n",
        "def _tops(db: str):\n",
        "    st=[]; out=[]\n",
        "    for idx,ch in enumerate(db,1):\n",
        "        if ch=='(':\n",
        "            st.append(idx)\n",
        "        elif ch==')':\n",
        "            if not st: continue\n",
        "            i=st.pop(); j=idx\n",
        "            if len(st)==0: out.append((i,j))\n",
        "    return out\n",
        "\n",
        "def onehot_phime_from_db(db: str)->np.ndarray:\n",
        "    L=len(db); P=np.zeros(L); H=np.zeros(L); I=np.zeros(L); M=np.zeros(L); E=np.zeros(L)\n",
        "    pt=_pair_table(db)\n",
        "    for i in range(1,L+1):\n",
        "        if pt[i]!=0: P[i-1]+=1\n",
        "    prev=0\n",
        "    for (i,j) in _tops(db):\n",
        "        for t in range(prev+1,i):\n",
        "            if pt[t]==0: E[t-1]+=1\n",
        "        _anno_pair(pt,i,j,H,I,M)\n",
        "        prev=j\n",
        "    for t in range(prev+1,L+1):\n",
        "        if pt[t]==0: E[t-1]+=1\n",
        "    return np.vstack([P,H,I,M,E]).T\n",
        "\n",
        "# ---------------- RNAsubopt ensemble (adaptive) ----------------\n",
        "\n",
        "def rnasubopt_structs(sequence: str, samples: int, temp_c: float = 37.0) -> List[str]:\n",
        "    seq=_to_rna(sequence); L=len(seq)\n",
        "    variants = [\n",
        "        (['RNAsubopt','-p',str(samples),'-T',f'{temp_c:.2f}','--stochBT'], f\">seq\\n{seq}\\n\"),\n",
        "        (['RNAsubopt','-p',str(samples),'--stochBT'],                          f\">seq\\n{seq}\\n\"),\n",
        "        (['RNAsubopt','-p',str(samples),'-T',f'{temp_c:.2f}'],                 seq+\"\\n\"),\n",
        "        (['RNAsubopt','-p',str(samples)],                                      seq+\"\\n\"),\n",
        "    ]\n",
        "    for cmd, inp in variants:\n",
        "        cp=_run(cmd, inp)\n",
        "        structs=_parse_structs(cp.stdout, L)\n",
        "        if structs: return structs\n",
        "    return []\n",
        "\n",
        "def subopt_loop_props_adaptive(sequence: str, temp_c: float = 37.0,\n",
        "                               batch: int = 200, max_samp: int = 1500,\n",
        "                               tol: float = 1e-2) -> np.ndarray:\n",
        "    \"\"\"Return ensemble average PHIME from adaptive RNAsubopt sampling.\"\"\"\n",
        "    seq=_to_rna(sequence); L=len(seq)\n",
        "    acc=np.zeros((L,5), dtype=float)\n",
        "    n=0\n",
        "\n",
        "    structs = rnasubopt_structs(seq, samples=batch, temp_c=temp_c)\n",
        "    if not structs:\n",
        "        out=np.zeros((L,5), np.float32); out[:,4]=1.0; return out\n",
        "    for db in structs:\n",
        "        acc += onehot_phime_from_db(db)\n",
        "    n += len(structs)\n",
        "\n",
        "    return (acc / float(n)).astype(np.float32)\n",
        "\n",
        "# ------------- RNAplfold: parse u_1..u_U --------------\n",
        "\n",
        "def _parse_lunp_matrix(path: str, L: int, U: int, strict: bool) -> Optional[np.ndarray]:\n",
        "    mat = np.full((L, U), np.nan, dtype=float)\n",
        "    row_idx = 0\n",
        "    with open(path, 'r') as f:\n",
        "        for ln in f:\n",
        "            ln = ln.strip()\n",
        "            if not ln or ln.startswith('#') or ln.startswith('>'):\n",
        "                continue\n",
        "            toks = ln.split()\n",
        "            nums=[]\n",
        "            for t in toks:\n",
        "                try: nums.append(float(t))\n",
        "                except: pass\n",
        "            if not nums:\n",
        "                continue\n",
        "            if len(nums) == U + 1:\n",
        "                i = int(round(nums[0]))\n",
        "                if 1 <= i <= L:\n",
        "                    mat[i-1, :] = nums[1:U+1]\n",
        "            elif len(nums) >= U:\n",
        "                if row_idx < L:\n",
        "                    mat[row_idx, :] = nums[-U:]\n",
        "                    row_idx += 1\n",
        "    if np.isnan(mat).any():\n",
        "        filled = np.count_nonzero(~np.isnan(mat))\n",
        "        if filled < L * max(1, U//2):\n",
        "            if strict:\n",
        "                raise RuntimeError(f\"RNAplfold parse incomplete for {path} (got {filled}/{L*U})\")\n",
        "            return None\n",
        "        for u in range(U):\n",
        "            col = mat[:,u]\n",
        "            mask = np.isnan(col)\n",
        "            if mask.any():\n",
        "                idx = np.where(~mask)[0]\n",
        "                if len(idx)==0: continue\n",
        "                last = col[idx[0]]\n",
        "                for i in range(L):\n",
        "                    if not mask[i]:\n",
        "                        last = col[i]\n",
        "                    else:\n",
        "                        col[i] = last\n",
        "                mat[:,u] = col\n",
        "    return np.clip(mat, 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "def rnaplfold_unpaired_multi(sequence: str, temp_c: float = 37.0,\n",
        "                             U: int = 9, W: Optional[int]=None,\n",
        "                             strict: bool = False) -> Optional[np.ndarray]:\n",
        "    seq=_to_rna(sequence); L=len(seq); W=W or L\n",
        "    tmp=tempfile.mkdtemp(prefix=\"plfold_\")\n",
        "    try:\n",
        "        cmd=['RNAplfold','-u',str(U),'-W',str(W),'-L',str(W),'-d','2','-T',f'{temp_c:.2f}',\n",
        "             '--auto-id','--id-prefix','phime','-c','0']\n",
        "        _run(cmd, f\">seq\\n{seq}\\n\", cwd=tmp)\n",
        "        candidates = sorted(glob.glob(os.path.join(tmp, \"phime_*_lunp\"))) or \\\n",
        "                     sorted(glob.glob(os.path.join(tmp, \"*_lunp\")))\n",
        "        for path in candidates:\n",
        "            mat=_parse_lunp_matrix(path, L, U, strict=strict)\n",
        "            if mat is not None and mat.shape==(L,U):\n",
        "                return mat\n",
        "        if strict:\n",
        "            raise RuntimeError(\"RNAplfold output not found or unparsable.\")\n",
        "        return None\n",
        "    finally:\n",
        "        shutil.rmtree(tmp, ignore_errors=True)\n",
        "\n",
        "# -------------------- Combined best-match --------------------\n",
        "\n",
        "def phime_plfold_calibrated(sequence: str,\n",
        "                            temp_c: float = 37.0,\n",
        "                            batch: int = 200,\n",
        "                            max_samp: int = 1500,\n",
        "                            tol: float = 1e-2,\n",
        "                            U: int = 9,\n",
        "                            smoothing: float = 0.02,\n",
        "                            strict_plfold: bool = False,\n",
        "                            return_u: bool = False) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
        "    seq=_to_rna(sequence); L=len(seq)\n",
        "    subopt = subopt_loop_props_adaptive(seq, temp_c=temp_c, batch=batch, max_samp=max_samp, tol=tol)  # [L,5]\n",
        "    u_all = rnaplfold_unpaired_multi(seq, temp_c=temp_c, U=U, W=L, strict=strict_plfold)\n",
        "    if u_all is None:\n",
        "        S = np.clip(subopt, 0, 1).astype(np.float32)\n",
        "        s = S.sum(axis=1, keepdims=True); s[s==0]=1.0\n",
        "        S = (S/s).astype(np.float32)\n",
        "        return (S, None) if return_u else (S, None)\n",
        "\n",
        "    u1 = u_all[:,0]\n",
        "    db_mfe,_ = rnafold_mfe(seq, temp_c=temp_c)\n",
        "    mfe = onehot_phime_from_db(db_mfe) if db_mfe else None\n",
        "\n",
        "    out = np.zeros((L,5), dtype=float)\n",
        "    eps = 1e-6\n",
        "    for i in range(L):\n",
        "        out[i,0] = max(0.0, 1.0 - float(u1[i]))  # P\n",
        "        u_sub = float(subopt[i,1:].sum())\n",
        "        if u_sub < eps:\n",
        "            prior = np.zeros(4, dtype=float)  # H,I,M,E\n",
        "            if mfe is not None and mfe[i,0] < 0.5:\n",
        "                k = int(np.argmax(mfe[i,1:]))\n",
        "                prior[k] = 1.0\n",
        "            else:\n",
        "                prior[3] = 1.0  # external\n",
        "            alpha = 1.0 - float(smoothing)\n",
        "            out[i,1:] = float(u1[i]) * (alpha*prior + smoothing*0.25)\n",
        "        else:\n",
        "            props = subopt[i,1:] / u_sub\n",
        "            out[i,1:] = props * float(u1[i])\n",
        "        rs = out[i].sum()\n",
        "        if rs <= 0:\n",
        "            out[i,4]=1.0\n",
        "        elif abs(rs-1.0) > 1e-8:\n",
        "            out[i] /= rs\n",
        "\n",
        "    S = out.astype(np.float32)\n",
        "    return (S, u_all) if return_u else (S, u_all)\n",
        "\n",
        "# -------------------------- Conversions --------------------------\n",
        "\n",
        "def phime_to_plum(S_phime: np.ndarray) -> np.ndarray:\n",
        "    P = S_phime[:, 0]\n",
        "    L = S_phime[:, 1]   # hairpin\n",
        "    U = S_phime[:, 4]   # external -> unstructured\n",
        "    M = S_phime[:, 2] + S_phime[:, 3]\n",
        "    S_plum = np.stack([P, L, U, M], axis=-1)\n",
        "    S_plum = S_plum / (S_plum.sum(axis=1, keepdims=True) + 1e-12)\n",
        "    return S_plum.astype(np.float32)\n",
        "\n",
        "def u_window_aggregates(u_all: Optional[np.ndarray], l_start:int=6, l_end:int=9):\n",
        "    if u_all is None:\n",
        "        return None, None\n",
        "    U = u_all.shape[1]\n",
        "    a = max(1, l_start) - 1\n",
        "    b = min(U, l_end)\n",
        "    if b <= a:\n",
        "        return None, None\n",
        "    slab = u_all[:, a:b]\n",
        "    return np.min(slab, axis=1).astype(np.float32), np.mean(slab, axis=1).astype(np.float32)\n",
        "\n",
        "# -------------------------- Public API (Notebook) --------------------------\n",
        "\n",
        "def compute_struct_for_sequence(seq: str,\n",
        "                                temp_c: float = 37.0,\n",
        "                                U: int = 9,\n",
        "                                batch: int = 200,\n",
        "                                max_samp: int = 1500,\n",
        "                                tol: float = 1e-2,\n",
        "                                smoothing: float = 0.02,\n",
        "                                strict_plfold: bool = False) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Return dict with:\n",
        "      PHIME [L,5], PLUM [L,4], u_min6_9 [L], u_mean6_9 [L], L [1], seq [1]\n",
        "    \"\"\"\n",
        "    S_phime, u_all = phime_plfold_calibrated(\n",
        "        seq, temp_c=temp_c, batch=batch, max_samp=max_samp, tol=tol,\n",
        "        U=U, smoothing=smoothing, strict_plfold=strict_plfold, return_u=True\n",
        "    )\n",
        "    umin, umean = u_window_aggregates(u_all, 6, 9)\n",
        "    if umin is None:\n",
        "        L = len(seq)\n",
        "        umin = np.zeros((L,), np.float32)\n",
        "        umean = np.zeros((L,), np.float32)\n",
        "    out = {\n",
        "        \"PHIME\": S_phime.astype(np.float32),\n",
        "        \"PLUM\": phime_to_plum(S_phime).astype(np.float32),\n",
        "        \"u_min6_9\": umin.astype(np.float32),\n",
        "        \"u_mean6_9\": umean.astype(np.float32),\n",
        "        \"L\": np.array([len(seq)], dtype=np.int32),\n",
        "        \"seq\": np.array([_to_rna(seq)], dtype=object),\n",
        "    }\n",
        "    return out\n",
        "\n",
        "def _valid_existing_npz(path: str, L: int) -> bool:\n",
        "    try:\n",
        "        with np.load(path) as z:\n",
        "            if \"PHIME\" not in z: return False\n",
        "            arr = z[\"PHIME\"]\n",
        "            return (arr.ndim == 2) and (arr.shape[0] == L) and (arr.shape[1] == 5)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _write_manifest(out_dir: str, n_total: int, dtype: str = \"float16\"):\n",
        "    manifest = {\n",
        "        \"key\": \"PHIME\",\n",
        "        \"channels\": [\"P\",\"H\",\"I\",\"M\",\"E\"],\n",
        "        \"pattern\": \"seq_%06d.npz\",\n",
        "        \"count\": int(n_total),\n",
        "        \"dtype\": dtype,\n",
        "        \"note\": \"Each NPZ has PHIME[L,5]. Index i corresponds to line i in the input sequences file.\"\n",
        "    }\n",
        "    with open(os.path.join(out_dir, \"FORMAT.json\"), \"w\") as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "\n",
        "def generate_struct_features(in_path: str, out_dir: str,\n",
        "                             workers: int = 4,\n",
        "                             temp_c: float = 37.0,\n",
        "                             U: int = 9,\n",
        "                             batch: int = 200,\n",
        "                             max_samp: int = 1500,\n",
        "                             tol: float = 1e-2,\n",
        "                             smoothing: float = 0.02,\n",
        "                             strict_plfold: bool = False,\n",
        "                             save_plum: bool = True,\n",
        "                             save_aggs: bool = True,\n",
        "                             dtype: str = \"float16\",\n",
        "                             start: Optional[int] = None,\n",
        "                             end: Optional[int] = None,\n",
        "                             resume: bool = True,\n",
        "                             verbose_every: int = 1000) -> Dict[str,int]:\n",
        "    \"\"\"\n",
        "    Read sequences (one per line) from `in_path`, write seq_%06d.npz with at least PHIME[L,5].\n",
        "    Returns summary dict with counts.\n",
        "    \"\"\"\n",
        "    if not check_vienna(verbose=True):\n",
        "        raise RuntimeError(\"ViennaRNA tools missing.\")\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Load all sequences\n",
        "    seqs=[]\n",
        "    with open(in_path, 'r') as f:\n",
        "        for ln in f:\n",
        "            s=_to_rna(ln.strip())\n",
        "            if s: seqs.append(s)\n",
        "    n_total = len(seqs)\n",
        "\n",
        "    # Shard\n",
        "    a = 0 if start is None else max(0, int(start))\n",
        "    b = n_total if end is None else min(n_total, int(end))\n",
        "    seqs_shard = seqs[a:b]\n",
        "    n = len(seqs_shard)\n",
        "    print(f\"Found {n_total} sequences; processing [{a}:{b}) → {n} items. Writing to: {out_dir}\")\n",
        "\n",
        "    # Worker function (defined at top-level so it's picklable)\n",
        "    def _worker(i: int, s: str):\n",
        "        try:\n",
        "            path = os.path.join(out_dir, f\"seq_{i:06d}.npz\")\n",
        "            L = len(s)\n",
        "            if resume and os.path.exists(path) and _valid_existing_npz(path, L):\n",
        "                return (i, True, \"cached\")\n",
        "            d = compute_struct_for_sequence(\n",
        "                s, temp_c=temp_c, U=U, batch=batch, max_samp=max_samp,\n",
        "                tol=tol, smoothing=smoothing, strict_plfold=strict_plfold\n",
        "            )\n",
        "            # trim optional keys if requested\n",
        "            np_dtype = np.float16 if str(dtype) == \"float16\" else np.float32\n",
        "            out = {\"PHIME\": d[\"PHIME\"].astype(np_dtype)}\n",
        "            if save_plum:\n",
        "                out[\"PLUM\"] = d[\"PLUM\"].astype(np_dtype)\n",
        "            if save_aggs:\n",
        "                out[\"u_min6_9\"] = d[\"u_min6_9\"].astype(np_dtype)\n",
        "                out[\"u_mean6_9\"] = d[\"u_mean6_9\"].astype(np_dtype)\n",
        "            out[\"L\"] = d[\"L\"]; out[\"seq\"] = d[\"seq\"]\n",
        "            np.savez_compressed(path, **out)\n",
        "            return (i, True, \"ok\")\n",
        "        except Exception as e:\n",
        "            return (i, False, str(e))\n",
        "\n",
        "    ok=0; fails=0\n",
        "    t0 = time.perf_counter()\n",
        "    workers = max(1, int(workers))\n",
        "\n",
        "    # If workers==1, run serially (useful for restricted notebook envs)\n",
        "    if workers == 1:\n",
        "        for idx, s in enumerate(seqs_shard, start=a):\n",
        "            i, success, msg = _worker(idx, s)\n",
        "            ok += int(success); fails += int(not success)\n",
        "            if ok % max(1, verbose_every) == 0:\n",
        "                print(f\"[{ok}/{n}] last={msg}\")\n",
        "    else:\n",
        "        with ProcessPoolExecutor(max_workers=workers) as ex:\n",
        "            futs = [ex.submit(_worker, idx, s) for idx, s in enumerate(seqs_shard, start=a)]\n",
        "            for fut in as_completed(futs):\n",
        "                i, success, msg = fut.result()\n",
        "                ok += int(success); fails += int(not success)\n",
        "                if ok % max(1, verbose_every) == 0:\n",
        "                    print(f\"[{ok}/{n}] last={msg}\")\n",
        "\n",
        "    _write_manifest(out_dir, n_total, dtype)\n",
        "    t1 = time.perf_counter()\n",
        "    print(f\"Done. OK: {ok}, Failed: {fails}. Time: {t1-t0:.1f}s\")\n",
        "    return {\"ok\": ok, \"failed\": fails, \"total\": n, \"start\": a, \"end\": b}\n",
        "\n",
        "def verify_struct_outputs(in_path: str, out_dir: str) -> Dict[str, object]:\n",
        "    \"\"\"Check that out_dir has valid NPZs for each line in the sequences file.\"\"\"\n",
        "    seqs=[]\n",
        "    with open(in_path, 'r') as f:\n",
        "        for ln in f:\n",
        "            s=_to_rna(ln.strip())\n",
        "            if s: seqs.append(s)\n",
        "    n = len(seqs)\n",
        "    missing=[]\n",
        "    ok=0\n",
        "    for i, s in enumerate(seqs):\n",
        "        p = os.path.join(out_dir, f\"seq_{i:06d}.npz\")\n",
        "        if os.path.exists(p) and _valid_existing_npz(p, len(s)):\n",
        "            ok += 1\n",
        "        else:\n",
        "            missing.append(i)\n",
        "    print(f\"Expected {n} NPZs. Found valid: {ok}. Missing/invalid: {len(missing)}.\")\n",
        "    if missing:\n",
        "        print(\"First few missing indices:\", missing[:20])\n",
        "    return {\"expected\": n, \"valid\": ok, \"missing\": missing}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMy_bxOzyjEh"
      },
      "outputs": [],
      "source": [
        "import rna_struct_features_jupyter as rsf\n",
        "\n",
        "# (optional) make sure ViennaRNA tools are on PATH\n",
        "rsf.check_vienna()\n",
        "\n",
        "# generate features for training RNAs\n",
        "rsf.generate_struct_features(\n",
        "    in_path=\"training_seqs.txt\",\n",
        "    out_dir=\"struct_train\",\n",
        "    workers=1,          # if you get issues in notebooks, set workers=1\n",
        "    dtype=\"float16\"     # saves disk; 'float32' also fine\n",
        ")\n",
        "\n",
        "# (optional) verify all NPZ files exist & look right\n",
        "rsf.verify_struct_outputs(\"training_seqs.txt\", \"struct_train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULzzmMt3F9Ev",
        "outputId": "3693725e-2474-4dbb-a866-91d14474d9f5"
      },
      "outputs": [],
      "source": [
        "# Zip the generated files in ./struct_train into struct_train.zip\n",
        "import shutil\n",
        "\n",
        "archive_path = shutil.make_archive(\"struct_train\", \"zip\", root_dir=\".\", base_dir=\"struct_train\")\n",
        "print(\"Created:\", archive_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SebrvL0YgZW",
        "outputId": "8b3d0937-cf2f-4507-a8a4-9e0e48660561"
      },
      "outputs": [],
      "source": [
        "# Unzip struct_train.zip into ./struct_train_unzipped\n",
        "import shutil\n",
        "\n",
        "shutil.unpack_archive(\"/content/struct_train.zip\", \"struct_train\", \"zip\")\n",
        "print(\"Extracted to: struct_train\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGzQtIkp-sMb"
      },
      "source": [
        "# model configs + utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td_bv8Aj1D0a",
        "outputId": "b6bb1444-b541-4f92-e334-f8c102ebb44a"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "RNA–RBP Binding Intensity Predictor (two-tower + ESM-2 ⊕ ProtT5 + cosine bilinear)\n",
        "===================================================================================\n",
        "\n",
        "- RBP-disjoint validation (unseen RBPs), same RNA/k-mer universe by default.\n",
        "- Protein side: frozen ESM-2 and ProtT5 embeddings (mean-pooled), concatenated, cached to .npy.\n",
        "- Optional fusion hygiene:\n",
        "    • PROT_SRC_ZSCORE: z-score each source (ESM, ProtT5) using TRAIN RBPs only\n",
        "    • PROT_SWAPDROP_P: with prob p, drop either source (never both) during TRAIN to encourage complementarity\n",
        "- Intensities are log1p + lightly clipped, then per-RBP z-scored (train RNAs).\n",
        "- Loss = 0.2 * Huber + 0.8 * (1 - Pearson per-RBP across RNAs in-batch).\n",
        "- Head: low-rank cosine bilinear (rank=256) for correlation-friendly scoring.\n",
        "\n",
        "- NEW: Optional PWM-based dedup of TRAIN RBPs from a JSON report (your clusters & representatives).\n",
        "  If CFG.DEDUP_PWM_JSON exists, training matrix and RBP list are sliced to representative columns.\n",
        "  A mapping CSV is written at CFG.DEDUP_WRITE_MAP. Embedding caches use a \"_dedup\" suffix.\n",
        "\n",
        "- NEW: Optional RNA secondary-structure features (PHIME: P,H,I,M,E) per base.\n",
        "  If CFG.RNA_USE_STRUCT=True, TrainData can load per-seq NPZs from CFG.RNA_STRUCT_DIR and\n",
        "  provide padded tensors via load_struct_batch(...). Later cells will pass these into RNATower.\n",
        "\n",
        "Author: You + ChatGPT (Aug 2025)\n",
        "\"\"\"\n",
        "\n",
        "import os, random, time, json, csv\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import T5Tokenizer, T5EncoderModel, AutoTokenizer, AutoModel\n",
        "\n",
        "# =============================\n",
        "# CONFIG (edit here)\n",
        "# =============================\n",
        "@dataclass\n",
        "class Config:\n",
        "    # File paths\n",
        "    TRAIN_MATRIX_PATH: str = 'training_data2.txt'\n",
        "    TRAIN_RBPS_PATH: str = 'training_RBPs2.txt'\n",
        "    TRAIN_SEQS_PATH: str = 'training_seqs.txt'\n",
        "\n",
        "    TEST_RBPS_PATH: str = 'test_RBPs2.txt'\n",
        "    TEST_SEQS_PATH: str = 'test_seqs.txt'\n",
        "\n",
        "    # --- EMA of model parameters ---\n",
        "    EMA_USE: bool = True          # turn EMA on/off\n",
        "    EMA_DECAY: float = 0.999      # typical: 0.999–0.9999 (higher = smoother, slower to react)\n",
        "    EMA_EVAL: bool = True         # use EMA weights for evaluation/checkpoints\n",
        "\n",
        "    # --- NEW: RNA structure features (PHIME: P,H,I,M,E) ---\n",
        "    RNA_USE_STRUCT: bool = True                 # flip to True after generating NPZs\n",
        "    RNA_STRUCT_DIR: Optional[str] = '/content/struct_train/struct_train'  # folder with seq_000000.npz, seq_000001.npz, ...\n",
        "    RNA_STRUCT_DIM: int = 5                      # PHIME has 5 channels\n",
        "    RNA_STRUCT_CACHE: int = 120678                   # LRU capacity (#seqs) for training-time cache\n",
        "\n",
        "    # --- (optional) training RBP dedup (PWM-based) ---\n",
        "    # If this JSON exists, we apply column dedup BEFORE computing mu/sd and embeddings.\n",
        "    # JSON schema: {\"clusters\":[{\"members\":[...], \"representative_col\": int}, ...], ...}\n",
        "    # Indices can be 0-based (preferred). If they look 1-based, we'll auto-shift.\n",
        "    DEDUP_PWM_JSON: Optional[str] = 'dedup_pwm_out/training_duplicate_clusters_pwm.json'\n",
        "    DEDUP_WRITE_MAP: str = 'cache/training_column_map.csv'  # old->new mapping\n",
        "\n",
        "    # Random seeds\n",
        "    SEED: int = 1\n",
        "\n",
        "    # Model dims\n",
        "    D_MODEL: int = 256\n",
        "    RNA_VOCAB: str = 'ACGU'      # T will be mapped to U\n",
        "    RANK: int = 512\n",
        "    GATE_STRENGTH: float = 0.5\n",
        "    # RNA tower\n",
        "    RNA_USE_TRANSFORMER: bool = True\n",
        "    RNA_TRANSFORMER_LAYERS: int = 2\n",
        "    RNA_NHEAD: int = 4\n",
        "    RNA_DROPOUT: float = 0.3\n",
        "    RNA_MAX_LEN: int = 64        # safety cap for positional embeddings\n",
        "\n",
        "    # Training\n",
        "    BATCH_RBPS: int = 8          # proteins per batch\n",
        "    BATCH_RNAS: int = 2048       # RNAs per batch\n",
        "    EPOCHS: int = 50\n",
        "    STEPS_PER_EPOCH: int = 300   # tune to time budget\n",
        "    LR: float = 1e-4\n",
        "    WEIGHT_DECAY: float = 1e-2\n",
        "    HUBER_DELTA: float = 1.0\n",
        "    CORR_WEIGHT: float = 0.8     # loss = (1-CORR_WEIGHT)*Huber + CORR_WEIGHT*(1-Pearson)\n",
        "    MIXED_PRECISION: bool = False\n",
        "\n",
        "    # Validation setup\n",
        "    USE_KMER_DISJOINT_RNA: bool = False\n",
        "    VAL_RBPS_COUNT: int = 0\n",
        "    VAL_RBPS_SEED: int = 176\n",
        "    VAL_RBPS_INDICES: Optional[List[int]] = None  # can be original indices; will auto-map if dedup applied\n",
        "\n",
        "    # (optional) different probe libraries\n",
        "    KMER_K: int = 9\n",
        "    VAL_KMER_FRACTION: float = 0.2\n",
        "\n",
        "    # Target preprocessing\n",
        "    LOG1P: bool = True\n",
        "    CLIP_PCTL: float = 99.5\n",
        "\n",
        "    # Evaluation / caching\n",
        "    EVAL_ZSCORE_TARGETS: bool = True\n",
        "    CACHE_DIR: str = 'cache'\n",
        "    SAVE_EVERY: int = 2\n",
        "\n",
        "    # ProtT5\n",
        "    USE_PROTT5: bool = False\n",
        "    PROTT5_MODEL_ID: str = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
        "    PROT_EMB_DIM: int = 1024           # overwritten if ESM-2 is added (→ 2304)\n",
        "    PROT_EMB_BATCH: int = 8\n",
        "    PROT_EMB_CACHE_TRAIN: str = 'cache/prot_emb_train.npy'\n",
        "    PROT_EMB_CACHE_TEST: str = 'cache/prot_emb_test.npy'   # unused here but kept for completeness\n",
        "\n",
        "    # ESM-2 (HF names supported; short names via map below)\n",
        "    USE_ESM2: bool = True\n",
        "    ESM2_MODEL_ID: str = \"facebook/esm2_t48_15B_UR50D\"\n",
        "    ESM_EMB_DIM: int = 1280            # info only\n",
        "    ESM_EMB_BATCH: int = 8\n",
        "    ESM_EMB_CACHE_TRAIN: str = 'cache/esm_emb_train.npy'\n",
        "    ESM_EMB_CACHE_TEST: str = 'cache/esm_emb_test.npy'     # unused here but kept for completeness\n",
        "\n",
        "    # Fusion hygiene / regularization\n",
        "    PROT_SRC_ZSCORE: bool = True\n",
        "    PROT_SWAPDROP_P: float = 0.0\n",
        "\n",
        "    # Prot MLP\n",
        "    PROT_MLP_HIDDEN: int = 256\n",
        "    PROT_DROPOUT: float = 0.3\n",
        "    PROT_MLP_WD: float = 3e-4\n",
        "\n",
        "CFG = Config()\n",
        "os.makedirs(CFG.CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# =============================\n",
        "# Reproducibility\n",
        "# =============================\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed(CFG.SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# =============================\n",
        "# Data loading & helpers\n",
        "# =============================\n",
        "def _read_lines(path: str) -> List[str]:\n",
        "    with open(path) as f:\n",
        "        return [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "def load_protein_seqs(path: str) -> List[str]:\n",
        "    return [s.upper() for s in _read_lines(path)]\n",
        "\n",
        "def load_rna_seqs(path: str) -> List[str]:\n",
        "    return [s.upper().replace('T', 'U') for s in _read_lines(path)]\n",
        "\n",
        "def load_matrix(path: str) -> np.ndarray:\n",
        "    return np.loadtxt(path, dtype=np.float32)\n",
        "\n",
        "def central_kmer(seq: str, k: int) -> str:\n",
        "    start = (len(seq) - k) // 2\n",
        "    return seq[start:start + k]\n",
        "\n",
        "def _with_suffix(path: str, suffix: str) -> str:\n",
        "    root, ext = os.path.splitext(path)\n",
        "    return root + suffix + (ext if ext else '')\n",
        "\n",
        "# =============================\n",
        "# NEW: PWM-JSON based TRAIN RBP dedup\n",
        "# =============================\n",
        "def _maybe_shift_to_zero_based(indices: List[int], M: int) -> List[int]:\n",
        "    \"\"\"If indices look 1-based (min>=1 and max==M), shift to 0-based.\"\"\"\n",
        "    if not indices: return indices\n",
        "    mn, mx = min(indices), max(indices)\n",
        "    if mn >= 1 and mx == M:\n",
        "        return [i - 1 for i in indices]\n",
        "    return indices\n",
        "\n",
        "def apply_rbp_dedup_by_json(\n",
        "    rbp_seqs: List[str],\n",
        "    Y: np.ndarray,\n",
        "    json_path: Optional[str],\n",
        "    map_csv_out: str\n",
        ") -> Tuple[List[str], np.ndarray, List[int], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Slice RBP columns of Y and rbp_seqs based on PWM cluster JSON.\n",
        "    Returns (rbp_seqs_dedup, Y_dedup, keep_idx, info).\n",
        "    If json missing/None -> returns inputs unchanged with info['applied']=False.\n",
        "    \"\"\"\n",
        "    M = Y.shape[1]\n",
        "    if not json_path or not os.path.exists(json_path):\n",
        "        print(\"[Dedup] No JSON provided or file missing → skipping.\")\n",
        "        keep_idx = list(range(M))\n",
        "        info = {\"applied\": False, \"n_clusters\": 0, \"n_groups_gt1\": 0, \"dropped\": 0}\n",
        "        return rbp_seqs, Y, keep_idx, info\n",
        "\n",
        "    with open(json_path, \"r\") as f:\n",
        "        report = json.load(f)\n",
        "    clusters = report.get(\"clusters\", [])\n",
        "    # Collect all mentioned indices, check base\n",
        "    mentioned = []\n",
        "    reps = []\n",
        "    for c in clusters:\n",
        "        mentioned.extend(c.get(\"members\", []))\n",
        "        reps.append(c.get(\"representative_col\"))\n",
        "    mentioned = _maybe_shift_to_zero_based(list(map(int, mentioned)), M)\n",
        "    reps = _maybe_shift_to_zero_based(list(map(int, reps)), M)\n",
        "\n",
        "    # Build rep mapping: default i->i\n",
        "    rep_of = list(range(M))\n",
        "    cluster_id_of = [-1] * M\n",
        "    group_sizes = []\n",
        "    for c, rep in zip(clusters, reps):\n",
        "        members = _maybe_shift_to_zero_based(list(map(int, c.get(\"members\", []))), M)\n",
        "        group_sizes.append(len(members))\n",
        "        for m in members:\n",
        "            if 0 <= m < M:\n",
        "                rep_of[m] = rep\n",
        "                cluster_id_of[m] = c.get(\"cluster_id\", -1)\n",
        "\n",
        "    # Keep the original column order of first appearance of each representative\n",
        "    first_seen = {}\n",
        "    for j in range(M):\n",
        "        r = rep_of[j]\n",
        "        if r not in first_seen:\n",
        "            first_seen[r] = j\n",
        "    keep_idx = sorted(first_seen.keys(), key=lambda r: first_seen[r])\n",
        "\n",
        "    dropped = M - len(keep_idx)\n",
        "    n_groups_gt1 = sum(1 for g in group_sizes if g > 1)\n",
        "\n",
        "    # Build old->new mapping (after dedup)\n",
        "    new_pos = {old_rep: i for i, old_rep in enumerate(keep_idx)}\n",
        "    os.makedirs(os.path.dirname(map_csv_out), exist_ok=True)\n",
        "    with open(map_csv_out, \"w\", newline=\"\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"old_col\", \"cluster_id\", \"representative_old_col\", \"kept_as_new_col\", \"dropped\"])\n",
        "        for j in range(M):\n",
        "            rep = rep_of[j]\n",
        "            kept = (j == rep)\n",
        "            w.writerow([\n",
        "                j,\n",
        "                cluster_id_of[j],\n",
        "                rep,\n",
        "                (new_pos[rep] if kept else \"\"),\n",
        "                int(not kept)\n",
        "            ])\n",
        "\n",
        "    print(\"[Dedup] Applied PWM JSON:\")\n",
        "    print(f\"   Original RBPs: {M} → Kept representatives: {len(keep_idx)} (dropped {dropped})\")\n",
        "    print(f\"   Clusters total: {len(clusters)} | with >1 member: {n_groups_gt1}\")\n",
        "    print(f\"   Mapping CSV: {map_csv_out}\")\n",
        "\n",
        "    # Slice data\n",
        "    Y2 = Y[:, keep_idx].copy()\n",
        "    rbp2 = [rbp_seqs[j] for j in keep_idx]\n",
        "    info = {\n",
        "        \"applied\": True,\n",
        "        \"keep_idx\": keep_idx,\n",
        "        \"rep_of\": rep_of,\n",
        "        \"cluster_id_of\": cluster_id_of,\n",
        "        \"n_clusters\": len(clusters),\n",
        "        \"n_groups_gt1\": n_groups_gt1,\n",
        "        \"dropped\": dropped\n",
        "    }\n",
        "    return rbp2, Y2, keep_idx, info\n",
        "\n",
        "# =============================\n",
        "# ProtT5 embedding (frozen)\n",
        "# =============================\n",
        "def load_or_compute_prott5_embeddings(\n",
        "    seqs: List[str], cache_path: str, model_id: str, batch_size: int, device: torch.device\n",
        ") -> np.ndarray:\n",
        "    if os.path.exists(cache_path):\n",
        "        E = np.load(cache_path)\n",
        "        if E.shape[0] == len(seqs):\n",
        "            print(f\"[ProtT5] Loaded cached embeddings: {cache_path} {E.shape}\")\n",
        "            return E.astype(np.float32)\n",
        "        print(f\"[ProtT5] Cache size mismatch ({E.shape[0]} vs {len(seqs)}), recomputing...\")\n",
        "\n",
        "    print(f\"[ProtT5] Computing embeddings with {model_id} ...\")\n",
        "    tok = T5Tokenizer.from_pretrained(model_id, legacy=True)\n",
        "    t5 = T5EncoderModel.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=(torch.float16 if device.type=='cuda' else torch.float32)\n",
        "    ).to(device).eval()\n",
        "\n",
        "    outs = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(seqs), batch_size):\n",
        "            batch = [\" \".join(s) for s in seqs[i:i+batch_size]]\n",
        "            enc = tok(batch, padding=True, return_tensors=\"pt\").to(device)\n",
        "            rep = t5(**enc).last_hidden_state                   # [B, L, 1024]\n",
        "            mask = enc[\"attention_mask\"].unsqueeze(-1)          # [B, L, 1]\n",
        "            pooled = (rep * mask).sum(1) / mask.sum(1)          # [B, 1024]\n",
        "            outs.append(pooled.float().cpu().numpy())\n",
        "            print(f\"  ProtT5 {min(i+batch_size, len(seqs))}/{len(seqs)}\")\n",
        "\n",
        "    E = np.concatenate(outs, axis=0).astype(np.float32)\n",
        "    np.save(cache_path, E)\n",
        "    print(f\"[ProtT5] Saved embeddings to {cache_path} with shape {E.shape}\")\n",
        "    del t5, tok\n",
        "    if device.type == 'cuda': torch.cuda.empty_cache()\n",
        "    return E\n",
        "\n",
        "# =============================\n",
        "# ESM-2 embedding (frozen, HF)\n",
        "# =============================\n",
        "_HF_ESM_MAP = {\n",
        "    \"esm2_t33_650M_UR50D\": \"facebook/esm2_t33_650M_UR50D\",\n",
        "    \"esm2_t36_3B_UR50D\": \"facebook/esm2_t36_3B_UR50D\",\n",
        "    \"esm2_t48_15B_UR50D\": \"facebook/esm2_t48_15B_UR50D\",\n",
        "}\n",
        "def _resolve_esm_id(model_id: str) -> str:\n",
        "    return model_id if model_id.startswith(\"facebook/\") else _HF_ESM_MAP.get(model_id, model_id)\n",
        "\n",
        "def load_or_compute_esm2_embeddings(\n",
        "    seqs: List[str], cache_path: str, model_id: str, batch_size: int, device: torch.device\n",
        ") -> np.ndarray:\n",
        "    if os.path.exists(cache_path):\n",
        "        E = np.load(cache_path)\n",
        "        if E.shape[0] == len(seqs):\n",
        "            print(f\"[ESM2] Loaded cached embeddings: {cache_path} {E.shape}\")\n",
        "            return E.astype(np.float32)\n",
        "        print(f\"[ESM2] Cache size mismatch ({E.shape[0]} vs {len(seqs)}), recomputing...\")\n",
        "\n",
        "    hf_model_name = _resolve_esm_id(model_id)\n",
        "    print(f\"[ESM2] Computing embeddings with {hf_model_name} ...\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
        "    model = AutoModel.from_pretrained(\n",
        "        hf_model_name,\n",
        "        torch_dtype=(torch.float16 if device.type=='cuda' else torch.float32)\n",
        "    ).to(device).eval()\n",
        "    print(f\"[ESM2] Model embedding dimension: {model.config.hidden_size}\")\n",
        "\n",
        "    outs = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(seqs), batch_size):\n",
        "            batch = seqs[i:i+batch_size]\n",
        "            inputs = tokenizer(batch, padding=True, truncation=True, max_length=1024, return_tensors=\"pt\").to(device)\n",
        "            hs = model(**inputs).last_hidden_state            # [B, L, D]\n",
        "            am = inputs[\"attention_mask\"]                     # [B, L]\n",
        "\n",
        "            pooled = []\n",
        "            for j in range(hs.size(0)):\n",
        "                seq_len = int(am[j].sum().item())\n",
        "                if seq_len > 2:\n",
        "                    aa = hs[j, 1:seq_len-1, :]                # exclude <cls> and <eos>\n",
        "                else:\n",
        "                    aa = hs[j, 1:seq_len, :]\n",
        "                pooled.append(aa.mean(0).float().cpu().numpy())\n",
        "            outs.append(np.stack(pooled, axis=0))\n",
        "\n",
        "            print(f\"  ESM2 {min(i+batch_size, len(seqs))}/{len(seqs)}\")\n",
        "            if device.type == 'cuda' and (i // batch_size) % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    E = np.concatenate(outs, axis=0).astype(np.float32)\n",
        "    np.save(cache_path, E)\n",
        "    print(f\"[ESM2] Saved embeddings to {cache_path} with shape {E.shape}\")\n",
        "    del model\n",
        "    if device.type == 'cuda': torch.cuda.empty_cache()\n",
        "    return E\n",
        "\n",
        "# =============================\n",
        "# Train data holder\n",
        "# =============================\n",
        "class TrainData:\n",
        "    \"\"\"Holds full training tensors and index splits; also saves μ,σ for z-scoring.\"\"\"\n",
        "    def __init__(self, cfg: Config):\n",
        "        self.cfg = cfg\n",
        "        print(\"\\n[Load] Reading training files ...\")\n",
        "        rna_seqs = load_rna_seqs(cfg.TRAIN_SEQS_PATH)\n",
        "        Y = load_matrix(cfg.TRAIN_MATRIX_PATH).astype(np.float32)   # (N_rna, N_rbp)\n",
        "        prot_seqs = load_protein_seqs(cfg.TRAIN_RBPS_PATH)\n",
        "\n",
        "        assert Y.shape[0] == len(rna_seqs), \"Rows of matrix must equal number of RNA sequences\"\n",
        "        assert Y.shape[1] == len(prot_seqs), \"Columns of matrix must equal number of RBP sequences\"\n",
        "        orig_M = Y.shape[1]\n",
        "\n",
        "        # ---- NEW: apply PWM-JSON dedup on TRAIN RBPs (columns) ----\n",
        "        prot_seqs, Y, keep_idx, dedup_info = apply_rbp_dedup_by_json(\n",
        "            prot_seqs, Y, cfg.DEDUP_PWM_JSON, cfg.DEDUP_WRITE_MAP\n",
        "        )\n",
        "        self.dedup_info = dedup_info\n",
        "        self.keep_idx = keep_idx  # old-col indices kept\n",
        "\n",
        "        if cfg.LOG1P:\n",
        "            clip = np.percentile(Y, cfg.CLIP_PCTL)\n",
        "            Y = np.log1p(np.clip(Y, None, clip))\n",
        "            print(f\"[Preproc] Applied log1p with clip@{cfg.CLIP_PCTL}p (<= {clip:.3f})\")\n",
        "\n",
        "        self.rna_seqs, self.Y, self.prot_seqs = rna_seqs, Y, prot_seqs\n",
        "        self.N_rna, self.N_rbp = len(rna_seqs), len(prot_seqs)\n",
        "        print(f\"    RNAs: {self.N_rna:,} | RBPs: {self.N_rbp} (from {orig_M} before dedup)\")\n",
        "\n",
        "        # ---- NEW: structure feature loader state ----\n",
        "        self.struct_dir = cfg.RNA_STRUCT_DIR if (cfg.RNA_USE_STRUCT and cfg.RNA_STRUCT_DIR) else None\n",
        "        self._struct_cache = OrderedDict()\n",
        "        self._struct_cache_cap = int(cfg.RNA_STRUCT_CACHE)\n",
        "\n",
        "        # RNA split\n",
        "        if cfg.USE_KMER_DISJOINT_RNA:\n",
        "            print(\"[Split] Building 9-mer-disjoint RNA split ...\")\n",
        "            kmers = [central_kmer(s, cfg.KMER_K) for s in rna_seqs]\n",
        "            uniq = list(sorted(set(kmers)))\n",
        "            rng = np.random.default_rng(cfg.SEED); rng.shuffle(uniq)\n",
        "            n_val_k = int(len(uniq) * cfg.VAL_KMER_FRACTION)\n",
        "            val_kset = set(uniq[:n_val_k])\n",
        "            train_idx, val_idx = zip(*[(i, None) if km not in val_kset else (None, i) for i, km in enumerate(kmers)])\n",
        "            self.train_idx = np.array([i for i in train_idx if i is not None], dtype=np.int64)\n",
        "            self.val_idx   = np.array([i for i in val_idx   if i is not None], dtype=np.int64)\n",
        "            print(f\"    Train RNAs: {len(self.train_idx):,} | Val RNAs: {len(self.val_idx):,} | Unique 9-mers: {len(uniq):,}\")\n",
        "        else:\n",
        "            print(\"[Split] Using ALL RNAs for train & val (same k-mer universe as test) ...\")\n",
        "            self.train_idx = np.arange(self.N_rna, dtype=np.int64)\n",
        "            self.val_idx   = self.train_idx.copy()\n",
        "            print(f\"    RNAs used: train={len(self.train_idx):,} | val={len(self.val_idx):,}\")\n",
        "\n",
        "        # RBP split\n",
        "        print(\"[Split] Building RBP-disjoint validation ...\")\n",
        "        all_rbps = np.arange(self.N_rbp)\n",
        "\n",
        "        if cfg.VAL_RBPS_INDICES is not None:\n",
        "            # If user supplied original indices, map them via representative mapping\n",
        "            orig_inds = np.array(cfg.VAL_RBPS_INDICES, dtype=int)\n",
        "            if dedup_info.get(\"applied\"):\n",
        "                rep_of = np.array(dedup_info[\"rep_of\"], dtype=int)\n",
        "                keep = np.array(self.keep_idx, dtype=int)\n",
        "                # Map each original to its representative, then to new position\n",
        "                rep_mapped = rep_of[orig_inds]\n",
        "                # new index = position in keep list\n",
        "                pos = {old_rep: i for i, old_rep in enumerate(keep)}\n",
        "                val_rbps = np.array([pos[r] for r in rep_mapped if r in pos], dtype=int)\n",
        "                val_rbps = np.unique(val_rbps)\n",
        "                if len(val_rbps) < len(orig_inds):\n",
        "                    print(f\"[Split] Warning: some supplied VAL_RBPS_INDICES collapsed under dedup (unique {len(val_rbps)}).\")\n",
        "            else:\n",
        "                val_rbps = np.unique(orig_inds)\n",
        "        else:\n",
        "            rng = np.random.default_rng(cfg.VAL_RBPS_SEED)\n",
        "            n_val = min(cfg.VAL_RBPS_COUNT, self.N_rbp)\n",
        "            val_rbps = rng.choice(all_rbps, size=n_val, replace=False)\n",
        "\n",
        "        self.train_rbps = np.setdiff1d(all_rbps, val_rbps).astype(np.int64)\n",
        "        self.val_rbps   = np.array(val_rbps, dtype=np.int64)\n",
        "        print(f\"    Train RBPs: {len(self.train_rbps)} | Val RBPs: {len(self.val_rbps)}\")\n",
        "        print(f\"    Held-out RBP indices (after dedup if applied): {self.val_rbps.tolist()}\")\n",
        "\n",
        "        # Per-RBP z-score stats from TRAIN RNAs\n",
        "        print(\"[Norm] Computing per-RBP z-score stats from TRAIN RNAs ...\")\n",
        "        Y_train = Y[self.train_idx, :]\n",
        "        self.mu = Y_train.mean(axis=0).astype(np.float32)\n",
        "        self.sd = Y_train.std(axis=0).astype(np.float32); self.sd[self.sd < 1e-6] = 1.0\n",
        "        np.save(os.path.join(cfg.CACHE_DIR, 'mu_train.npy'), self.mu)\n",
        "        np.save(os.path.join(cfg.CACHE_DIR, 'sd_train.npy'), self.sd)\n",
        "\n",
        "    # ---- structure feature helpers (PHIME) ----\n",
        "    def _struct_path(self, idx: int) -> str:\n",
        "        return os.path.join(self.struct_dir, f\"seq_{idx:06d}.npz\")\n",
        "\n",
        "    def _load_struct_one(self, idx: int, L_expected: int) -> np.ndarray:\n",
        "        C = CFG.RNA_STRUCT_DIM\n",
        "        if not self.struct_dir:\n",
        "            S = np.zeros((L_expected, C), np.float32); S[:, 4] = 1.0\n",
        "            return S\n",
        "\n",
        "        key = (int(idx), int(L_expected))\n",
        "        if key in self._struct_cache:\n",
        "            val = self._struct_cache.pop(key)\n",
        "            self._struct_cache[key] = val\n",
        "            return val\n",
        "\n",
        "        S = None\n",
        "        loaded = False\n",
        "        src_name = \"fallback\"\n",
        "        try:\n",
        "            with np.load(self._struct_path(idx)) as npz:\n",
        "                if \"PHIME\" in npz:\n",
        "                    S = np.array(npz[\"PHIME\"], dtype=np.float32)  # (L,5)\n",
        "                    loaded = True\n",
        "                    src_name = \"PHIME\"\n",
        "                elif \"PLUM\" in npz:\n",
        "                    P, Lc, Uc, Mc = (np.array(npz[\"PLUM\"], dtype=np.float32).T)\n",
        "                    I = 0.5 * Mc\n",
        "                    M2 = Mc - I\n",
        "                    S = np.stack([P, Lc, I, M2, Uc], axis=-1).astype(np.float32)\n",
        "                    loaded = True\n",
        "                    src_name = \"PLUM→PHIME\"\n",
        "        except Exception:\n",
        "            S = None\n",
        "\n",
        "        if S is None:\n",
        "            S = np.zeros((L_expected, C), np.float32); S[:, 4] = 1.0\n",
        "\n",
        "        # One-time confirmation print *only* if a real NPZ was loaded\n",
        "        if loaded and not getattr(self, \"_struct_confirm_printed\", False):\n",
        "            print(f\"[Struct] Loaded {src_name} for seq_{idx:06d} from {self._struct_path(idx)}; shape={S.shape}\")\n",
        "            self._struct_confirm_printed = True\n",
        "\n",
        "        # length fixups (unchanged from your version) ...\n",
        "        if S.shape[0] != L_expected:\n",
        "            if S.shape[0] > L_expected:\n",
        "                S = S[:L_expected]\n",
        "            else:\n",
        "                pad = np.zeros((L_expected - S.shape[0], S.shape[1]), np.float32)\n",
        "                S = np.vstack([S, pad])\n",
        "\n",
        "        # cache put (unchanged) ...\n",
        "        self._struct_cache[key] = S\n",
        "        if len(self._struct_cache) > self._struct_cache_cap:\n",
        "            self._struct_cache.popitem(last=False)\n",
        "        return S\n",
        "\n",
        "\n",
        "    def load_struct_batch(self, indices: np.ndarray, max_len: Optional[int] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Build a [B, Lmax, 5] tensor aligned to tokenize_rna_batch padding.\n",
        "        Returns CPU tensor (float32). Caller can .to(device).\n",
        "        \"\"\"\n",
        "        if indices is None or len(indices) == 0:\n",
        "            return torch.zeros((0, 0, CFG.RNA_STRUCT_DIM), dtype=torch.float32)\n",
        "        lens = [len(self.rna_seqs[int(i)]) for i in indices]\n",
        "        Lmax = max_len or (max(lens) if lens else 0)\n",
        "        B = len(indices); C = CFG.RNA_STRUCT_DIM\n",
        "        out = np.zeros((B, Lmax, C), np.float32)\n",
        "        for b, (i, L) in enumerate(zip(indices, lens)):\n",
        "            S = self._load_struct_one(int(i), L)\n",
        "            out[b, :L, :] = S\n",
        "        return torch.from_numpy(out)\n",
        "\n",
        "# =============================\n",
        "# Tokenization & batching\n",
        "# =============================\n",
        "# ---- RNA vocab with a real PAD id (single source of truth) ----\n",
        "PAD_ID = len(CFG.RNA_VOCAB)                # = 4 when CFG.RNA_VOCAB == 'ACGU'\n",
        "RNA_TO_IDX = {ch: i for i, ch in enumerate(CFG.RNA_VOCAB)}  # real tokens are 0..3\n",
        "\n",
        "def tokenize_rna_batch(seqs: List[str]) -> torch.Tensor:\n",
        "    lens = [len(s) for s in seqs]\n",
        "    max_len = max(lens) if lens else 0\n",
        "    out = torch.full((len(seqs), max_len), fill_value=PAD_ID, dtype=torch.long)  # fill with PAD\n",
        "    for i, s in enumerate(seqs):\n",
        "        ids = [RNA_TO_IDX.get(ch, 0) for ch in s]  # unknowns → 'A' (0), fine for RNAcompete\n",
        "        if ids:\n",
        "            out[i, :len(ids)] = torch.tensor(ids, dtype=torch.long)\n",
        "    return out\n",
        "\n",
        "class BatchSampler:\n",
        "    \"\"\"Yields (rbp_idx_list, rna_idx_list) for each training step.\"\"\"\n",
        "    def __init__(self, data: TrainData, cfg: Config):\n",
        "        self.data, self.cfg = data, cfg\n",
        "        self.rng = np.random.default_rng(cfg.SEED)\n",
        "    def sample(self) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        rbps = self.rng.choice(self.data.train_rbps, size=CFG.BATCH_RBPS, replace=False)\n",
        "        rnas = self.rng.choice(self.data.train_idx,   size=CFG.BATCH_RNAS, replace=False)\n",
        "        return rbps.astype(np.int64), rnas.astype(np.int64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUsNCnHM-wpj"
      },
      "source": [
        "## model components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2lJW61R1IYX"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# Model components\n",
        "# =============================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim: int, max_len: int):\n",
        "        super().__init__()\n",
        "        self.pe = nn.Embedding(max_len, dim)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Clamp positions to avoid OOB if a sequence exceeds max_len\n",
        "        L = x.size(1)\n",
        "        pos = torch.arange(L, device=x.device)\n",
        "        pos = pos.clamp_max(self.pe.num_embeddings - 1).unsqueeze(0)\n",
        "        return x + self.pe(pos)\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, dim: int, kernel: int, dilation: int, dropout: float):\n",
        "        super().__init__()\n",
        "        padding = (kernel - 1) // 2 * dilation\n",
        "        self.conv = nn.Conv1d(dim, dim, kernel_size=kernel, padding=padding,\n",
        "                              dilation=dilation, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln = nn.LayerNorm(dim)\n",
        "    def forward(self, x: torch.Tensor, mask_1d: torch.Tensor) -> torch.Tensor:\n",
        "        y = self.conv(x)                         # [B, D, L]\n",
        "        y = F.gelu(y)\n",
        "        y = y.transpose(1, 2)                    # [B, L, D]\n",
        "        y = self.ln(y)\n",
        "        y = y.transpose(1, 2)\n",
        "        y = self.dropout(y)\n",
        "        out = x + y\n",
        "        return out * mask_1d.unsqueeze(1).to(out.dtype)\n",
        "\n",
        "class GatedPooling(nn.Module):\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim, 1)\n",
        "        self.alpha = nn.Parameter(torch.tensor(0.2))\n",
        "        self.log_sigma = nn.Parameter(torch.log(torch.tensor(6.0)))\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        scores = self.proj(x).squeeze(-1)                        # [B,L]\n",
        "        B, L = scores.size()\n",
        "        pos = torch.arange(L, device=x.device).float().unsqueeze(0).expand(B, -1)\n",
        "        lens = (mask.sum(1) if mask is not None else torch.full((B,), L, device=x.device)).clamp(min=1).float().unsqueeze(1)\n",
        "        centers = (lens - 1) / 2\n",
        "        dist2 = (pos - centers).pow(2)\n",
        "        sigma = torch.exp(self.log_sigma) + 1e-6\n",
        "        center_bias = - dist2 / (2 * sigma * sigma)\n",
        "        scores = scores + self.alpha * center_bias\n",
        "        if mask is not None:\n",
        "            neg = torch.finfo(scores.dtype).min\n",
        "            scores = scores.masked_fill(~mask.bool(), neg)\n",
        "        attn = torch.softmax(scores.float(), dim=1).to(x.dtype)\n",
        "        return torch.einsum('bl,bld->bd', attn, x)\n",
        "\n",
        "class ProtMLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden: int, out_dim: int, p: float):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden), nn.GELU(), nn.LayerNorm(hidden), nn.Dropout(p),\n",
        "            nn.Linear(hidden, out_dim),\n",
        "        )\n",
        "        self.out_norm = nn.LayerNorm(out_dim)\n",
        "        self._init()\n",
        "    def _init(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.out_norm(self.net(x))\n",
        "\n",
        "class RNATower(nn.Module):\n",
        "    \"\"\"\n",
        "    Tokens: LongTensor [B, L]\n",
        "    Optional structure (PHIME): FloatTensor [B, L, 5] where channels = [P,H,I,M,E].\n",
        "    If provided, structure is projected to model dim and added residually to token embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        V, D = len(CFG.RNA_VOCAB), cfg.D_MODEL\n",
        "        self.embed = nn.Embedding(V + 1, D, padding_idx=PAD_ID)\n",
        "        self.pos = PositionalEncoding(D, cfg.RNA_MAX_LEN)\n",
        "\n",
        "        # --- NEW: structure fusion (light residual) ---\n",
        "        self.use_struct = bool(getattr(cfg, \"RNA_USE_STRUCT\", False))\n",
        "        if self.use_struct:\n",
        "            self.struct_proj = nn.Linear(getattr(cfg, \"RNA_STRUCT_DIM\", 5), D, bias=True)\n",
        "            self.struct_ln = nn.LayerNorm(D)\n",
        "            self.struct_drop = nn.Dropout(cfg.RNA_DROPOUT)\n",
        "            self.struct_scale = nn.Parameter(torch.tensor(1.0))\n",
        "        else:\n",
        "            self.struct_proj = None\n",
        "\n",
        "        self.conv1 = ConvBlock(D, kernel=5,  dilation=1, dropout=cfg.RNA_DROPOUT)\n",
        "        self.conv2 = ConvBlock(D, kernel=9,  dilation=2, dropout=cfg.RNA_DROPOUT)\n",
        "        self.conv3 = ConvBlock(D, kernel=13, dilation=4, dropout=cfg.RNA_DROPOUT)\n",
        "        self.k9 = nn.Conv1d(D, D, kernel_size=9, padding=4, bias=False)\n",
        "        self.k9_gamma = nn.Parameter(torch.tensor(0.5))\n",
        "        if cfg.RNA_USE_TRANSFORMER:\n",
        "            el = nn.TransformerEncoderLayer(\n",
        "                d_model=D, nhead=cfg.RNA_NHEAD, dim_feedforward=D*4,\n",
        "                dropout=cfg.RNA_DROPOUT, batch_first=True\n",
        "            )\n",
        "            self.tf = nn.TransformerEncoder(el, num_layers=cfg.RNA_TRANSFORMER_LAYERS)\n",
        "        else:\n",
        "            self.tf = None\n",
        "        self.pool = GatedPooling(D)\n",
        "        self.out_norm = nn.LayerNorm(D)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, struct: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        mask = (tokens != PAD_ID)                           # [B,L]\n",
        "        x = self.embed(tokens)                              # [B,L,D]\n",
        "        x = self.pos(x)\n",
        "        x = x * mask.unsqueeze(-1).to(x.dtype)\n",
        "\n",
        "        # Inject structure if available + enabled\n",
        "        if self.use_struct and struct is not None and self.struct_proj is not None:\n",
        "            # struct: [B,L,5]; zero is safe for PAD since loader pads with zeros\n",
        "            s = self.struct_proj(struct.to(x.dtype))        # [B,L,D]\n",
        "            s = self.struct_ln(F.gelu(s))\n",
        "            s = self.struct_drop(s)\n",
        "            x = x + self.struct_scale * s\n",
        "            # keep masking clean\n",
        "            x = x * mask.unsqueeze(-1).to(x.dtype)\n",
        "\n",
        "        xc = x.transpose(1, 2)                              # [B,D,L]\n",
        "        xc = self.conv1(xc, mask)\n",
        "        xc = self.conv2(xc, mask)\n",
        "        xc = self.conv3(xc, mask)\n",
        "        k9 = F.gelu(self.k9(xc))\n",
        "        k9 = k9 * mask.unsqueeze(1).to(k9.dtype)\n",
        "        xc = xc + self.k9_gamma * k9\n",
        "        x = xc.transpose(1, 2)\n",
        "        if self.tf is not None:\n",
        "            x = self.tf(x, src_key_padding_mask=~mask)\n",
        "        h = self.pool(x, mask)\n",
        "        return self.out_norm(h)\n",
        "\n",
        "class GatedBilinearLowRankCosine(nn.Module):\n",
        "    \"\"\"\n",
        "    Cosine bilinear with protein-conditioned gating in rank space.\n",
        "    Gate is residual & zero-centered: scale axes by (1 + s * tanh(G e_p)),\n",
        "    so with GATE_STRENGTH=0 you recover the original head.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, rank: int, gate_strength: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.U = nn.Linear(dim, rank, bias=False)   # protein -> rank\n",
        "        self.V = nn.Linear(dim, rank, bias=False)   # RNA -> rank\n",
        "        self.G = nn.Linear(dim, rank, bias=True)    # protein -> gate params\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "        self.gate_strength = gate_strength\n",
        "\n",
        "    def forward(self, e_p: torch.Tensor, e_r: torch.Tensor) -> torch.Tensor:\n",
        "        # e_p: [Bp, D], e_r: [Br, D]\n",
        "        up = F.normalize(self.U(e_p), dim=1)        # [Bp, R]\n",
        "        vr = F.normalize(self.V(e_r), dim=1)        # [Br, R]\n",
        "        g  = torch.tanh(self.G(e_p))                # [-1,1], shape [Bp, R]\n",
        "        upg = up * (1.0 + self.gate_strength * g)   # protein-conditional scaling of rank axes\n",
        "        return upg @ vr.t() + self.bias             # [Bp, Br]\n",
        "\n",
        "\n",
        "class TwoTowerModel(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.rna = RNATower(cfg)\n",
        "        self.prot_proj = ProtMLP(cfg.PROT_EMB_DIM, cfg.PROT_MLP_HIDDEN, cfg.D_MODEL, cfg.PROT_DROPOUT)\n",
        "        self.score = GatedBilinearLowRankCosine(cfg.D_MODEL, cfg.RANK, cfg.GATE_STRENGTH)\n",
        "\n",
        "    def encode_rna(self, rna_tokens: torch.Tensor, rna_struct: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        return self.rna(rna_tokens, rna_struct)\n",
        "\n",
        "    def project_prot(self, prot_vecs: torch.Tensor) -> torch.Tensor:\n",
        "        return self.prot_proj(prot_vecs)\n",
        "\n",
        "    def forward_scores_vecs(\n",
        "        self,\n",
        "        prot_vecs: torch.Tensor,\n",
        "        rna_tokens: torch.Tensor,\n",
        "        rna_struct: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        e_p = self.project_prot(prot_vecs)\n",
        "        e_r = self.encode_rna(rna_tokens, rna_struct)\n",
        "        return self.score(e_p, e_r)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmDyBpUr-y_g"
      },
      "source": [
        "## training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIhnFqvD1M6p",
        "outputId": "444c4896-3e31-4062-e266-1af454061010"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# Losses & metrics\n",
        "# =============================\n",
        "class PearsonLoss(nn.Module):\n",
        "    \"\"\"1 - Pearson, averaged across protein rows in the mini-batch.\"\"\"\n",
        "    def __init__(self, eps: float = 1e-8): super().__init__(); self.eps = eps\n",
        "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        pred = pred - pred.mean(dim=1, keepdim=True)\n",
        "        target = target - target.mean(dim=1, keepdim=True)\n",
        "        cov = (pred * target).sum(dim=1)\n",
        "        denom = torch.sqrt((pred.pow(2).sum(dim=1) + self.eps) * (target.pow(2).sum(dim=1) + self.eps))\n",
        "        r = cov / denom\n",
        "        return (1.0 - r).mean()\n",
        "\n",
        "def huber_loss(pred: torch.Tensor, target: torch.Tensor, delta: float) -> torch.Tensor:\n",
        "    return F.huber_loss(pred, target, delta=delta)\n",
        "\n",
        "def pearson_numpy(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    a = a - a.mean(); b = b - b.mean()\n",
        "    denom = np.sqrt((a*a).sum() * (b*b).sum()) + 1e-12\n",
        "    return float((a*b).sum() / denom)\n",
        "\n",
        "# =============================\n",
        "# Training / evaluation helpers\n",
        "# =============================\n",
        "def build_param_groups(model: nn.Module):\n",
        "    prot_decay, decay, no_decay = [], [], []\n",
        "    for module_name, module in model.named_modules():\n",
        "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
        "            for name, param in module.named_parameters(recurse=False):\n",
        "                if not param.requires_grad: continue\n",
        "                if name == \"weight\":\n",
        "                    (prot_decay if module_name.startswith(\"prot_proj\") else decay).append(param)\n",
        "                else:\n",
        "                    no_decay.append(param)\n",
        "        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n",
        "            for _, param in module.named_parameters(recurse=False):\n",
        "                if param.requires_grad: no_decay.append(param)\n",
        "    covered = {id(p) for p in prot_decay + decay + no_decay}\n",
        "    for _, p in model.named_parameters():\n",
        "        if p.requires_grad and id(p) not in covered: decay.append(p)\n",
        "    return {\"prot_decay\": prot_decay, \"decay\": decay, \"no_decay\": no_decay}\n",
        "\n",
        "# --- NEW: Exponential Moving Average of model params ---\n",
        "import copy\n",
        "\n",
        "class ModelEMA:\n",
        "    \"\"\"\n",
        "    Track an exponential moving average of model parameters & buffers.\n",
        "    Keeps a full copy of the model as `module` in eval mode with requires_grad=False.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: nn.Module, decay: float = 0.999, device: Optional[torch.device] = None):\n",
        "        self.decay = float(decay)\n",
        "        if device is None:\n",
        "            device = next(model.parameters()).device\n",
        "        self.module = copy.deepcopy(model).to(device).eval()\n",
        "        for p in self.module.parameters():\n",
        "            p.requires_grad_(False)\n",
        "        self.num_updates = 0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, model: nn.Module):\n",
        "        \"\"\"EMA: m_ema = decay * m_ema + (1 - decay) * m.\"\"\"\n",
        "        self.num_updates += 1\n",
        "        ema_sd = self.module.state_dict()\n",
        "        mdl_sd = model.state_dict()\n",
        "        for k, ema_v in ema_sd.items():\n",
        "            v = mdl_sd[k]\n",
        "            if not torch.is_floating_point(v):\n",
        "                ema_v.copy_(v)  # buffers (ints/bools) copy directly\n",
        "            else:\n",
        "                ema_v.mul_(self.decay).add_(v, alpha=1.0 - self.decay)\n",
        "\n",
        "    def to(self, device: torch.device):\n",
        "        self.module.to(device)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return {\n",
        "            \"module\": self.module.state_dict(),\n",
        "            \"decay\": self.decay,\n",
        "            \"num_updates\": self.num_updates,\n",
        "        }\n",
        "\n",
        "    def load_state_dict(self, state):\n",
        "        self.module.load_state_dict(state[\"module\"])\n",
        "        self.decay = float(state.get(\"decay\", self.decay))\n",
        "        self.num_updates = int(state.get(\"num_updates\", 0))\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, data: TrainData, cfg: Config):\n",
        "        self.data, self.cfg = data, cfg\n",
        "\n",
        "        # Choose cache names (avoid overwriting non-dedup caches)\n",
        "        cache_suffix = \"_dedup\" if data.dedup_info.get(\"applied\") else \"\"\n",
        "        pt5_cache = _with_suffix(cfg.PROT_EMB_CACHE_TRAIN, cache_suffix)\n",
        "        esm_cache = _with_suffix(cfg.ESM_EMB_CACHE_TRAIN, cache_suffix)\n",
        "\n",
        "        # --- Protein embeddings (ProtT5 and/or ESM-2) ---\n",
        "        self.pt5_all = None\n",
        "        self.esm_all = None\n",
        "        total_dim = 0\n",
        "\n",
        "        if cfg.USE_PROTT5:\n",
        "            pt5_np = load_or_compute_prott5_embeddings(\n",
        "                data.prot_seqs, pt5_cache, cfg.PROTT5_MODEL_ID, cfg.PROT_EMB_BATCH, device\n",
        "            )\n",
        "            self.pt5_all = torch.tensor(pt5_np, dtype=torch.float32, device=device)  # [N_rbp, 1024]\n",
        "            total_dim += self.pt5_all.shape[1]\n",
        "\n",
        "        if cfg.USE_ESM2:\n",
        "            esm_np = load_or_compute_esm2_embeddings(\n",
        "                data.prot_seqs, esm_cache, cfg.ESM2_MODEL_ID, cfg.ESM_EMB_BATCH, device\n",
        "            )\n",
        "            self.esm_all = torch.tensor(esm_np, dtype=torch.float32, device=device)  # [N_rbp, 1280+]\n",
        "            total_dim += self.esm_all.shape[1]\n",
        "\n",
        "        if total_dim == 0:\n",
        "            raise ValueError(\"At least one of USE_PROTT5 or USE_ESM2 must be True.\")\n",
        "        cfg.PROT_EMB_DIM = total_dim\n",
        "\n",
        "        used = [name for name, arr in ((\"ESM2\", self.esm_all), (\"ProtT5\", self.pt5_all)) if arr is not None]\n",
        "        print(f\"[Prot] Using {', '.join(used)} → PROT_EMB_DIM={cfg.PROT_EMB_DIM}\")\n",
        "\n",
        "        # Build model (uses cfg.PROT_EMB_DIM)\n",
        "        self.model = TwoTowerModel(cfg).to(device)\n",
        "\n",
        "        # --- NEW: EMA (safe defaults if not in Config) ---\n",
        "        ema_use   = bool(getattr(cfg, \"EMA_USE\", True))\n",
        "        ema_decay = float(getattr(cfg, \"EMA_DECAY\", 0.999))\n",
        "        self.ema = ModelEMA(self.model, decay=ema_decay) if ema_use else None\n",
        "\n",
        "        # Train-only source-wise standardization stats\n",
        "        if cfg.PROT_SRC_ZSCORE:\n",
        "            tr = torch.tensor(self.data.train_rbps, device=device)\n",
        "\n",
        "            if self.pt5_all is not None:\n",
        "                self.pt5_mu = self.pt5_all[tr].mean(0, keepdim=True)\n",
        "                self.pt5_sd = self.pt5_all[tr].std(0, keepdim=True).clamp_min(1e-6)\n",
        "            else:\n",
        "                self.pt5_mu = self.pt5_sd = None\n",
        "\n",
        "            if self.esm_all is not None:\n",
        "                self.esm_mu = self.esm_all[tr].mean(0, keepdim=True)\n",
        "                self.esm_sd = self.esm_all[tr].std(0, keepdim=True).clamp_min(1e-6)\n",
        "            else:\n",
        "                self.esm_mu = self.esm_sd = None\n",
        "        else:\n",
        "            self.pt5_mu = self.pt5_sd = self.esm_mu = self.esm_sd = None\n",
        "\n",
        "        # Optimizer with parameter groups\n",
        "        groups = build_param_groups(self.model)\n",
        "        print(f\"[Opt] Param groups — prot_decay: {sum(p.numel() for p in groups['prot_decay']):,} | \"\n",
        "              f\"decay: {sum(p.numel() for p in groups['decay']):,} | \"\n",
        "              f\"no_decay: {sum(p.numel() for p in groups['no_decay']):,}\")\n",
        "\n",
        "        self.opt = torch.optim.AdamW([\n",
        "            {\"params\": groups[\"prot_decay\"], \"lr\": cfg.LR, \"weight_decay\": cfg.PROT_MLP_WD},\n",
        "            {\"params\": groups[\"decay\"],      \"lr\": cfg.LR, \"weight_decay\": cfg.WEIGHT_DECAY},\n",
        "            {\"params\": groups[\"no_decay\"],   \"lr\": cfg.LR, \"weight_decay\": 0.0},\n",
        "        ])\n",
        "\n",
        "        # AMP (new API)\n",
        "        self.scaler = torch.amp.GradScaler('cuda', enabled=cfg.MIXED_PRECISION and device.type == 'cuda')\n",
        "        self.pearson_loss = PearsonLoss()\n",
        "        self.best_median = -1.0\n",
        "        self.checkpoint_path = os.path.join(cfg.CACHE_DIR, 'best_model.pt' + ('.dedup' if cache_suffix else ''))\n",
        "\n",
        "\n",
        "    def _prep_prot(self, rbp_idx: torch.Tensor, training: bool) -> torch.Tensor:\n",
        "        \"\"\"Return per-RBP protein features (concat of available sources in [ESM, ProtT5] order).\"\"\"\n",
        "        # ESM2 branch (optional)\n",
        "        esm = None\n",
        "        if self.esm_all is not None:\n",
        "            esm = self.esm_all[rbp_idx]\n",
        "            if self.cfg.PROT_SRC_ZSCORE and (self.esm_mu is not None) and (self.esm_sd is not None):\n",
        "                esm = (esm - self.esm_mu) / self.esm_sd\n",
        "\n",
        "        # ProtT5 branch (optional)\n",
        "        pt5 = None\n",
        "        if self.pt5_all is not None:\n",
        "            pt5 = self.pt5_all[rbp_idx]\n",
        "            if self.cfg.PROT_SRC_ZSCORE and (self.pt5_mu is not None) and (self.pt5_sd is not None):\n",
        "                pt5 = (pt5 - self.pt5_mu) / self.pt5_sd\n",
        "\n",
        "        # Sanity: at least one must be present (guarded in __init__, but keep it here too)\n",
        "        if esm is None and pt5 is None:\n",
        "            raise RuntimeError(\"No protein embeddings available: set USE_ESM2 and/or USE_PROTT5 to True.\")\n",
        "\n",
        "        # Optional swapdrop (only meaningful if both streams exist)\n",
        "        if training and self.cfg.PROT_SWAPDROP_P > 0 and (esm is not None) and (pt5 is not None):\n",
        "            p = self.cfg.PROT_SWAPDROP_P\n",
        "            B = esm.size(0)\n",
        "            m_esm = (torch.rand(B, 1, device=esm.device) < p)\n",
        "            m_pt5 = (torch.rand(B, 1, device=pt5.device) < p)\n",
        "            # Avoid dropping both: if both true, drop only ESM (keep ProtT5)\n",
        "            both = m_esm & m_pt5\n",
        "            m_pt5 = m_pt5 & ~both\n",
        "            esm = esm.masked_fill(m_esm, 0.0)\n",
        "            pt5 = pt5.masked_fill(m_pt5, 0.0)\n",
        "\n",
        "        # Concatenate in [ESM, ProtT5] order when both exist\n",
        "        if esm is not None and pt5 is not None:\n",
        "            return torch.cat([esm, pt5], dim=1)\n",
        "        return esm if esm is not None else pt5\n",
        "\n",
        "\n",
        "    def _maybe_struct_batch(self, rna_idx: np.ndarray) -> Optional[torch.Tensor]:\n",
        "        \"\"\"Fetch PHIME batch if available & enabled; shape [B, L, 5] padded with zeros.\"\"\"\n",
        "        if not getattr(self.cfg, \"RNA_USE_STRUCT\", False):\n",
        "            return None\n",
        "        if not hasattr(self.data, \"load_struct_batch\"):\n",
        "            return None\n",
        "        S = self.data.load_struct_batch(rna_idx)  # expected torch.FloatTensor or np.ndarray\n",
        "        if S is None:\n",
        "            return None\n",
        "        if isinstance(S, np.ndarray):\n",
        "            S = torch.from_numpy(S)\n",
        "        return S  # device move happens at call site\n",
        "\n",
        "    def sample_batch(self, sampler: BatchSampler):\n",
        "        rbp_idx_np, rna_idx = sampler.sample()\n",
        "        rbp_idx = torch.tensor(rbp_idx_np, device=device)\n",
        "        prot_vecs = self._prep_prot(rbp_idx, training=True)                          # [Bp, in_dim]\n",
        "        rna_tokens = tokenize_rna_batch([self.data.rna_seqs[i] for i in rna_idx]).to(device)\n",
        "        rna_struct = self._maybe_struct_batch(rna_idx)\n",
        "        if rna_struct is not None: rna_struct = rna_struct.to(device)\n",
        "\n",
        "        mu = torch.tensor(self.data.mu[rbp_idx_np], device=device).unsqueeze(1)      # [Bp,1]\n",
        "        sd = torch.tensor(self.data.sd[rbp_idx_np], device=device).unsqueeze(1)      # [Bp,1]\n",
        "        Y = torch.tensor(self.data.Y[np.ix_(rna_idx, rbp_idx_np)], device=device).t()# [Bp,Br]\n",
        "        return prot_vecs, rna_tokens, rna_struct, (Y - mu) / sd\n",
        "\n",
        "    def train(self):\n",
        "        sampler = BatchSampler(self.data, self.cfg)\n",
        "        for epoch in range(1, self.cfg.EPOCHS + 1):\n",
        "            self.model.train(); losses = []; t0 = time.time()\n",
        "            for step in range(1, self.cfg.STEPS_PER_EPOCH + 1):\n",
        "                prot_vecs, rna_tokens, rna_struct, Yz = self.sample_batch(sampler)\n",
        "                self.opt.zero_grad(set_to_none=True)\n",
        "                with torch.amp.autocast('cuda', enabled=self.cfg.MIXED_PRECISION and device.type == 'cuda'):\n",
        "                    S = self.model.forward_scores_vecs(prot_vecs, rna_tokens, rna_struct)  # [Bp, Br]\n",
        "                    loss = (1 - self.cfg.CORR_WEIGHT) * huber_loss(S, Yz, delta=self.cfg.HUBER_DELTA) \\\n",
        "                           + self.cfg.CORR_WEIGHT * self.pearson_loss(S, Yz)\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.opt); self.scaler.update()\n",
        "\n",
        "                # --- EMA update after optimizer step ---\n",
        "                if self.ema is not None:\n",
        "                    self.ema.update(self.model)\n",
        "\n",
        "                losses.append(loss.item())\n",
        "                if step % 50 == 0:\n",
        "                    print(f\"Epoch {epoch} Step {step}/{self.cfg.STEPS_PER_EPOCH} | loss={np.mean(losses[-50:]):.4f}\")\n",
        "\n",
        "            med_r, mean_r = self.evaluate()\n",
        "            print(f\"[Eval] Epoch {epoch} | median r={med_r:.4f} | mean r={mean_r:.4f} | time={time.time()-t0:.1f}s\")\n",
        "\n",
        "            if med_r > self.best_median:\n",
        "                self.best_median = med_r\n",
        "                payload = {\n",
        "                    'model': self.model.state_dict(),\n",
        "                    'ema': (self.ema.state_dict() if self.ema is not None else None),\n",
        "                    'cfg': self.cfg.__dict__,\n",
        "                    'median_r': med_r,\n",
        "                }\n",
        "                torch.save(payload, self.checkpoint_path)\n",
        "                print(f\"[Save] New best median r={med_r:.4f}. Saved to {self.checkpoint_path}\")\n",
        "\n",
        "            if epoch % self.cfg.SAVE_EVERY == 0:\n",
        "                payload = {\n",
        "                    'model': self.model.state_dict(),\n",
        "                    'ema': (self.ema.state_dict() if self.ema is not None else None),\n",
        "                    'cfg': self.cfg.__dict__,\n",
        "                    'epoch': epoch,\n",
        "                }\n",
        "                torch.save(payload, os.path.join(self.cfg.CACHE_DIR, f'checkpoint_epoch_{epoch}.pt'))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, use_tta=True, n_augments=5, noise_scale=0.01) -> Tuple[float, float]:\n",
        "        \"\"\"Pearson per-RBP across RNAs.\n",
        "          If no held-out RBPs exist, evaluate on TRAIN RBPs/RNAs instead of returning 0.\"\"\"\n",
        "        # Decide which split to use\n",
        "        use_val = hasattr(self.data, \"val_rbps\") and len(self.data.val_rbps) > 0\n",
        "        rbp_idx_np = self.data.val_rbps if use_val else self.data.train_rbps\n",
        "        rna_idx_np = self.data.val_idx   if (use_val and hasattr(self.data, \"val_idx\")) else self.data.train_idx\n",
        "        split_name = \"VAL\" if use_val else \"TRAIN\"\n",
        "        print(f\"[Eval] Using {split_name} split (RBPs={len(rbp_idx_np)}, RNAs={len(rna_idx_np)})\")\n",
        "\n",
        "        # Pick model (EMA or current)\n",
        "        use_ema_for_eval = bool(getattr(self.cfg, \"EMA_EVAL\", True))\n",
        "        eval_model = self.ema.module if (self.ema is not None and use_ema_for_eval) else self.model\n",
        "        eval_model.eval()\n",
        "\n",
        "        # Tokenize RNAs for this split\n",
        "        tokens = tokenize_rna_batch([self.data.rna_seqs[i] for i in rna_idx_np])\n",
        "\n",
        "        # Optional structure\n",
        "        struct = None\n",
        "        if getattr(self.cfg, \"RNA_USE_STRUCT\", False) and hasattr(self.data, \"load_struct_batch\"):\n",
        "            vs = self.data.load_struct_batch(rna_idx_np)\n",
        "            if isinstance(vs, np.ndarray): vs = torch.from_numpy(vs)\n",
        "            struct = vs\n",
        "\n",
        "        # Encode RNAs\n",
        "        rna_emb = self._encode_rna_in_batches(tokens, struct, model=eval_model)  # [N_rna, D]\n",
        "\n",
        "        # Protein features\n",
        "        rbp_idx_t = torch.tensor(rbp_idx_np, device=device)\n",
        "        base = self._prep_prot(rbp_idx_t, training=False)  # [N_rbp, in_dim]\n",
        "\n",
        "        # TTA (optional)\n",
        "        if use_tta:\n",
        "            all_scores = []\n",
        "            for k in range(n_augments):\n",
        "                prot_vecs = base if k == 0 else base + torch.randn_like(base) * noise_scale\n",
        "                e_p = eval_model.project_prot(prot_vecs)\n",
        "                all_scores.append(eval_model.score(e_p, rna_emb).detach().cpu().numpy())\n",
        "            S = np.mean(all_scores, axis=0)  # [N_rbp, N_rna]\n",
        "        else:\n",
        "            e_p = eval_model.project_prot(base)\n",
        "            S = eval_model.score(e_p, rna_emb).detach().cpu().numpy()\n",
        "\n",
        "        # Targets for the same split\n",
        "        Y = self.data.Y[np.ix_(rna_idx_np, rbp_idx_np)]\n",
        "        if self.cfg.EVAL_ZSCORE_TARGETS:\n",
        "            mu = self.data.mu[rbp_idx_np][None, :]\n",
        "            sd = self.data.sd[rbp_idx_np][None, :]\n",
        "            Y = (Y - mu) / sd\n",
        "        Y = Y.T  # [N_rbp, N_rna]\n",
        "\n",
        "        # Per-RBP Pearson\n",
        "        rs = np.zeros(len(rbp_idx_np), dtype=np.float32)\n",
        "        for i in range(len(rbp_idx_np)):\n",
        "            rs[i] = pearson_numpy(S[i], Y[i])\n",
        "        return float(np.median(rs)), float(rs.mean())\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _encode_rna_in_batches(\n",
        "        self,\n",
        "        tokens: torch.Tensor,\n",
        "        struct: Optional[torch.Tensor] = None,\n",
        "        batch: int = 2048,\n",
        "        model: Optional[nn.Module] = None,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        tokens: [N, L]\n",
        "        struct: optional [N, L, 5] aligned & zero-padded; may be on CPU.\n",
        "        model: which model to use for encoding (EMA or current)\n",
        "        returns: [N, D] on `device`\n",
        "        \"\"\"\n",
        "        model = model or self.model\n",
        "        model.eval(); outs = []\n",
        "        N = tokens.size(0)\n",
        "        for i in range(0, N, batch):\n",
        "            tok = tokens[i:i+batch].to(device)\n",
        "            st = None\n",
        "            if struct is not None:\n",
        "                st = struct[i:i+batch]\n",
        "                if isinstance(st, np.ndarray): st = torch.from_numpy(st)\n",
        "                st = st.to(device)\n",
        "            outs.append(model.encode_rna(tok, st))\n",
        "        return torch.cat(outs, dim=0)\n",
        "\n",
        "# =============================\n",
        "# Main\n",
        "# =============================\n",
        "def main():\n",
        "    data = TrainData(CFG)\n",
        "    trainer = Trainer(data, CFG)\n",
        "    trainer.train()\n",
        "    print(\"\\n[Info] Best validation median r = {:.4f}\".format(trainer.best_median))\n",
        "    print(f\"Best checkpoint: {trainer.checkpoint_path}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svKOwO4t-_Eu"
      },
      "source": [
        "## calculate test rbp embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "a17605889dd542b58561ed3d429b04f2",
            "d60d22d81ca44b26b5f0a699dfe01625",
            "b1c01b0e274f4267923ad3e38174645d",
            "3b2ba58f4374405fbb74a05d6df453f7",
            "3eaadd85cede40baab72cbd2792107d5",
            "4c570ea71a274386b6dbff27afaf734e",
            "f81db617bf28457597351803b3ab4a92",
            "6a5176bc0a4a471583cb0343af500f91",
            "9ee5697e28de4579a98f4e29cb3819af",
            "553bea2b980b4769874dc456b2653131",
            "371e040eb39748cfa8c134a7c1db322c"
          ]
        },
        "id": "8UGg_IrVo9EU",
        "outputId": "d6bf3224-8392-4be2-9155-7de2503240a2"
      },
      "outputs": [],
      "source": [
        "# %% Colab cell: ESM-2 embeddings for test RBPs (no CLI args) + index sidecar\n",
        "# If needed: !pip -q install transformers\n",
        "\n",
        "import os, sys, gc, numpy as np, torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "RBP_FILE    = \"test_RBPs2.txt\"                    # one protein per line\n",
        "OUT_PATH    = \"cache/esm_emb_test.npy\"            # embeddings (N, D)\n",
        "INDEX_PATH  = \"cache/esm_emb_test.index.txt\"      # 0-based index per row (one int per line)\n",
        "MODEL_ID    = \"facebook/esm2_t48_15B_UR50D\"       # 15B\n",
        "FALLBACK_ON_OOM = False\n",
        "FALLBACK_MODEL  = \"facebook/esm2_t33_650M_UR50D\"  # fallback if OOM\n",
        "BATCH_SIZE  = 4                                   # tune if you see OOM\n",
        "OVERWRITE   = False                               # set True to force recompute\n",
        "MAX_LEN     = 1024                                # tokenizer cap (ESM-2 limit)\n",
        "# ------------------------------------------\n",
        "\n",
        "def read_lines(path):\n",
        "    with open(path) as f:\n",
        "        return [ln.strip().upper() for ln in f if ln.strip()]\n",
        "\n",
        "def pick_device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def pick_dtype(device):\n",
        "    if device.type == \"cuda\":\n",
        "        return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "    return torch.float32\n",
        "\n",
        "def load_model(model_id, dtype, device):\n",
        "    print(f\"[ESM2] Loading {model_id} (dtype={dtype}, device={device}) ...\")\n",
        "    tok = AutoTokenizer.from_pretrained(model_id)\n",
        "    mdl = AutoModel.from_pretrained(model_id, torch_dtype=dtype)\n",
        "    mdl = mdl.to(device).eval()\n",
        "    print(f\"[ESM2] Hidden size: {mdl.config.hidden_size}\")\n",
        "    return tok, mdl\n",
        "\n",
        "def clear_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_embeddings(seqs, tokenizer, model, device, batch_size, max_len):\n",
        "    N = len(seqs)\n",
        "    outs = []\n",
        "    for i in range(0, N, batch_size):\n",
        "        batch = seqs[i:i+batch_size]\n",
        "        inputs = tokenizer(\n",
        "            batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "        hs = model(**inputs).last_hidden_state         # [B, L, D]\n",
        "        am = inputs[\"attention_mask\"]                  # [B, L]\n",
        "        pooled = []\n",
        "        for j in range(hs.size(0)):\n",
        "            L = int(am[j].sum().item())\n",
        "            toks = hs[j, 1:L-1, :] if L > 2 else hs[j, 1:L, :]  # drop <cls>/<eos> if both present\n",
        "            pooled.append(toks.mean(0).float().cpu().numpy())\n",
        "        outs.append(np.stack(pooled, axis=0))\n",
        "        if (i // batch_size) % 10 == 0:\n",
        "            print(f\"[ESM2] {min(i+batch_size, N)}/{N}\")\n",
        "        if device.type == \"cuda\" and ((i // batch_size) % 8 == 0):\n",
        "            clear_cuda()\n",
        "    return np.concatenate(outs, axis=0).astype(np.float32)\n",
        "\n",
        "# ---- run ----\n",
        "os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
        "seqs = read_lines(RBP_FILE)\n",
        "N = len(seqs)\n",
        "print(f\"[Load] RBPs: {N}\")\n",
        "\n",
        "if os.path.exists(OUT_PATH) and not OVERWRITE:\n",
        "    try:\n",
        "        E_existing = np.load(OUT_PATH)\n",
        "        if E_existing.shape[0] == N:\n",
        "            # Still (re)write the index file to be safe/consistent\n",
        "            with open(INDEX_PATH, \"w\") as f:\n",
        "                for i in range(N):\n",
        "                    f.write(f\"{i}\\n\")\n",
        "            print(f\"[Skip] Existing embeddings match rows at {OUT_PATH} ({E_existing.shape}).\")\n",
        "            print(f\"[Info] Wrote index sidecar: {INDEX_PATH} (0-based, one per line)\")\n",
        "            raise SystemExit\n",
        "    except Exception:\n",
        "        pass  # fall through to recompute\n",
        "\n",
        "device = pick_device()\n",
        "dtype  = pick_dtype(device)\n",
        "print(f\"[Device] {device}\")\n",
        "\n",
        "try:\n",
        "    tokenizer, model = load_model(MODEL_ID, dtype, device)\n",
        "    E = compute_embeddings(seqs, tokenizer, model, device, BATCH_SIZE, MAX_LEN)\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e).lower() and FALLBACK_ON_OOM:\n",
        "        print(\"\\n[OOM] GPU ran out of memory with 15B. Falling back to\", FALLBACK_MODEL)\n",
        "        del model; clear_cuda(); gc.collect()\n",
        "        tokenizer, model = load_model(FALLBACK_MODEL, dtype, device)\n",
        "        E = compute_embeddings(seqs, tokenizer, model, device, BATCH_SIZE*2, MAX_LEN)\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "np.save(OUT_PATH, E)\n",
        "print(f\"[Done] Saved embeddings: {OUT_PATH} with shape {E.shape}\")\n",
        "\n",
        "# Sidecar: 0-based index for each embedding row (row i ↔ line i in RBP_FILE)\n",
        "with open(INDEX_PATH, \"w\") as f:\n",
        "    for i in range(N):\n",
        "        f.write(f\"{i}\\n\")\n",
        "print(f\"[Done] Saved index sidecar: {INDEX_PATH} (0-based, one per line)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9inwABjrqfzY",
        "outputId": "436663f2-25dc-4341-d2f7-879ba6f29061"
      },
      "outputs": [],
      "source": [
        "import numpy as np, pathlib\n",
        "\n",
        "E = np.load(\"cache/esm_emb_test.npy\")                  # [N, D]\n",
        "idx = np.loadtxt(\"cache/esm_emb_test.index.txt\", dtype=int)  # [N]\n",
        "\n",
        "print(E.shape, idx.shape)           # e.g., (44, 1280) (44,)\n",
        "assert (idx == np.arange(len(idx))).all()   # mapping is 0..N-1\n",
        "\n",
        "# Optional: verify line counts vs your RBP file\n",
        "N_file = sum(1 for _ in open(\"test_RBPs2.txt\") if _.strip())\n",
        "assert E.shape[0] == N_file == idx.size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC5CUMy9_QpR"
      },
      "source": [
        "## prediction code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwXvbSkbpXx7",
        "outputId": "36b2f0b6-71a3-4a24-81c6-ebeb7c4bd3e1"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# predict_streaming_standalone.py (with TTA)\n",
        "# Streams RNAs, loads PHIME per batch, uses ESM-2 test embeddings, writes RBP201.. files.\n",
        "\n",
        "import os, numpy as np, torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List\n",
        "\n",
        "# ==============================\n",
        "# Hardcoded paths & knobs\n",
        "# ==============================\n",
        "RBP_FILE   = \"test_RBPs2.txt\"              # one protein per line (44 lines)\n",
        "RNA_FILE   = \"test_seqs.txt\"               # one RNA per line (same order as outputs)\n",
        "CKPT_PATH  = \"cache/best_model.pt.dedup\"   # your final checkpoint\n",
        "STRUCT_DIR = \"struct_test\"                 # where seq_000000.npz ... live (PHIME)\n",
        "ESM_TEST   = \"cache/esm_emb_test.npy\"      # precomputed ESM-2 embeddings for test RBPs\n",
        "ESM_TRAIN  = \"cache/esm_emb_train.npy\"     # optional: train z-score (if enabled during training)\n",
        "\n",
        "OUT_DIR    = \"predictions\"                 # output folder\n",
        "BASE_OFFSET = 200                          # → RBP201.txt .. RBP(200+P).txt\n",
        "RNA_BATCH   = 4096                         # RNA batch size for streaming\n",
        "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "FMT = \"%.8f\"                               # eight decimals\n",
        "\n",
        "# --- TTA knobs (match training evaluate) ---\n",
        "TTA_USE   = True\n",
        "TTA_N     = 5\n",
        "TTA_NOISE = 0.01                           # noise std in (z-scored) ESM feature space\n",
        "\n",
        "# ==============================\n",
        "# Minimal config & constants\n",
        "# ==============================\n",
        "@dataclass\n",
        "class Config:\n",
        "    # (training cfg fields we need; rest carried from checkpoint's cfg)\n",
        "    RNA_USE_STRUCT: bool = True\n",
        "    RNA_STRUCT_DIM: int = 5\n",
        "    RNA_MAX_LEN: int = 64\n",
        "    RNA_VOCAB: str = 'ACGU'\n",
        "\n",
        "    D_MODEL: int = 256\n",
        "    RANK: int = 512\n",
        "    PROT_EMB_DIM: int = 1280\n",
        "\n",
        "    RNA_USE_TRANSFORMER: bool = True\n",
        "    RNA_NHEAD: int = 4\n",
        "    RNA_TRANSFORMER_LAYERS: int = 2\n",
        "    RNA_DROPOUT: float = 0.3\n",
        "\n",
        "    GATE_STRENGTH: float = 0.5\n",
        "\n",
        "    USE_ESM2: bool = True\n",
        "    USE_PROTT5: bool = False\n",
        "    PROT_SRC_ZSCORE: bool = True\n",
        "\n",
        "    PROT_MLP_HIDDEN: int = 256\n",
        "    PROT_DROPOUT: float = 0.3\n",
        "\n",
        "# ==============================\n",
        "# Helpers\n",
        "# ==============================\n",
        "def read_lines(path: str, normalize_rna: bool=False) -> List[str]:\n",
        "    xs=[]\n",
        "    with open(path) as f:\n",
        "        for ln in f:\n",
        "            s = ln.strip()\n",
        "            if not s: continue\n",
        "            s = s.upper()\n",
        "            if normalize_rna: s = s.replace(\"T\",\"U\")\n",
        "            xs.append(s)\n",
        "    return xs\n",
        "\n",
        "def build_vocab(cfg: Config):\n",
        "    vocab = {ch: i for i, ch in enumerate(cfg.RNA_VOCAB)}\n",
        "    pad_id = len(cfg.RNA_VOCAB)\n",
        "    return vocab, pad_id\n",
        "\n",
        "def tokenize_rna_batch(cfg: Config, seqs: List[str]) -> torch.Tensor:\n",
        "    vocab, pad_id = build_vocab(cfg)\n",
        "    lens = [len(s) for s in seqs]\n",
        "    Lmax = max(lens) if lens else 0\n",
        "    out = torch.full((len(seqs), Lmax), fill_value=pad_id, dtype=torch.long)\n",
        "    for i, s in enumerate(seqs):\n",
        "        ids = [vocab.get(ch, 0) for ch in s]  # unknowns→A(0)\n",
        "        if ids:\n",
        "            out[i, :len(ids)] = torch.tensor(ids, dtype=torch.long)\n",
        "    return out\n",
        "\n",
        "# ==============================\n",
        "# Model (mirrors your training shapes)\n",
        "# ==============================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim: int, max_len: int):\n",
        "        super().__init__()\n",
        "        self.pe = nn.Embedding(max_len, dim)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        L = x.size(1)\n",
        "        pos = torch.arange(L, device=x.device).clamp_max(self.pe.num_embeddings - 1).unsqueeze(0)\n",
        "        return x + self.pe(pos)\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, dim: int, kernel: int, dilation: int, dropout: float):\n",
        "        super().__init__()\n",
        "        padding = (kernel - 1) // 2 * dilation\n",
        "        self.conv = nn.Conv1d(dim, dim, kernel_size=kernel, padding=padding, dilation=dilation, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln = nn.LayerNorm(dim)\n",
        "    def forward(self, x: torch.Tensor, mask_1d: torch.Tensor) -> torch.Tensor:\n",
        "        y = self.conv(x)                  # [B,D,L]\n",
        "        y = F.gelu(y).transpose(1, 2)     # [B,L,D]\n",
        "        y = self.ln(y).transpose(1, 2)\n",
        "        y = self.dropout(y)\n",
        "        out = x + y\n",
        "        return out * mask_1d.unsqueeze(1).to(out.dtype)\n",
        "\n",
        "class GatedPooling(nn.Module):\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim, 1)\n",
        "        self.alpha = nn.Parameter(torch.tensor(0.2))\n",
        "        self.log_sigma = nn.Parameter(torch.log(torch.tensor(6.0)))\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        scores = self.proj(x).squeeze(-1)            # [B,L]\n",
        "        B, L = scores.size()\n",
        "        pos = torch.arange(L, device=x.device).float().unsqueeze(0).expand(B, -1)\n",
        "        lens = (mask.sum(1) if mask is not None else torch.full((B,), L, device=x.device)).clamp(min=1).float().unsqueeze(1)\n",
        "        centers = (lens - 1) / 2\n",
        "        dist2 = (pos - centers).pow(2)\n",
        "        sigma = torch.exp(self.log_sigma) + 1e-6\n",
        "        scores = scores + self.alpha * (- dist2 / (2 * sigma * sigma))\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(~mask.bool(), torch.finfo(scores.dtype).min)\n",
        "        attn = torch.softmax(scores.float(), dim=1).to(x.dtype)\n",
        "        return torch.einsum('bl,bld->bd', attn, x)\n",
        "\n",
        "class ProtMLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden: int, out_dim: int, p: float):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden), nn.GELU(), nn.LayerNorm(hidden), nn.Dropout(p),\n",
        "            nn.Linear(hidden, out_dim),\n",
        "        )\n",
        "        self.out_norm = nn.LayerNorm(out_dim)\n",
        "        self._init()\n",
        "    def _init(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.out_norm(self.net(x))\n",
        "\n",
        "class RNATower(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.vocab, self.pad_id = build_vocab(cfg)\n",
        "        V, D = len(cfg.RNA_VOCAB), cfg.D_MODEL\n",
        "        self.embed = nn.Embedding(V + 1, D, padding_idx=self.pad_id)\n",
        "        self.pos = PositionalEncoding(D, cfg.RNA_MAX_LEN)\n",
        "\n",
        "        self.use_struct = bool(getattr(cfg, \"RNA_USE_STRUCT\", False))\n",
        "        if self.use_struct:\n",
        "            self.struct_proj = nn.Linear(getattr(cfg, \"RNA_STRUCT_DIM\", 5), D, bias=True)\n",
        "            self.struct_ln = nn.LayerNorm(D)\n",
        "            self.struct_drop = nn.Dropout(cfg.RNA_DROPOUT)\n",
        "            self.struct_scale = nn.Parameter(torch.tensor(1.0))\n",
        "        else:\n",
        "            self.struct_proj = None\n",
        "\n",
        "        self.conv1 = ConvBlock(D, 5, 1, cfg.RNA_DROPOUT)\n",
        "        self.conv2 = ConvBlock(D, 9, 2, cfg.RNA_DROPOUT)\n",
        "        self.conv3 = ConvBlock(D, 13, 4, cfg.RNA_DROPOUT)\n",
        "        self.k9 = nn.Conv1d(D, D, kernel_size=9, padding=4, bias=False)\n",
        "        self.k9_gamma = nn.Parameter(torch.tensor(0.5))\n",
        "\n",
        "        if cfg.RNA_USE_TRANSFORMER:\n",
        "            el = nn.TransformerEncoderLayer(d_model=D, nhead=cfg.RNA_NHEAD, dim_feedforward=D*4,\n",
        "                                            dropout=cfg.RNA_DROPOUT, batch_first=True)\n",
        "            self.tf = nn.TransformerEncoder(el, num_layers=cfg.RNA_TRANSFORMER_LAYERS)\n",
        "        else:\n",
        "            self.tf = None\n",
        "\n",
        "        self.pool = GatedPooling(D)\n",
        "        self.out_norm = nn.LayerNorm(D)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, struct: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        mask = (tokens != self.pad_id)                      # [B,L]\n",
        "        x = self.embed(tokens)                              # [B,L,D]\n",
        "        x = self.pos(x)\n",
        "        x = x * mask.unsqueeze(-1).to(x.dtype)\n",
        "\n",
        "        if self.use_struct and struct is not None and self.struct_proj is not None:\n",
        "            s = self.struct_proj(struct.to(x.dtype))\n",
        "            s = self.struct_ln(F.gelu(s))\n",
        "            s = self.struct_drop(s)\n",
        "            x = x + self.struct_scale * s\n",
        "            x = x * mask.unsqueeze(-1).to(x.dtype)\n",
        "\n",
        "        xc = x.transpose(1, 2)                              # [B,D,L]\n",
        "        xc = self.conv1(xc, mask)\n",
        "        xc = self.conv2(xc, mask)\n",
        "        xc = self.conv3(xc, mask)\n",
        "        k9 = F.gelu(self.k9(xc))\n",
        "        k9 = k9 * mask.unsqueeze(1).to(k9.dtype)\n",
        "        xc = xc + self.k9_gamma * k9\n",
        "        x = xc.transpose(1, 2)\n",
        "\n",
        "        if self.tf is not None:\n",
        "            x = self.tf(x, src_key_padding_mask=~mask)\n",
        "        h = self.pool(x, mask)\n",
        "        return self.out_norm(h)\n",
        "\n",
        "class GatedBilinearLowRankCosine(nn.Module):\n",
        "    def __init__(self, dim: int, rank: int, gate_strength: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.U = nn.Linear(dim, rank, bias=False)   # protein -> rank\n",
        "        self.V = nn.Linear(dim, rank, bias=False)   # RNA -> rank\n",
        "        self.G = nn.Linear(dim, rank, bias=True)    # protein -> gate\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "        self.gate_strength = gate_strength\n",
        "    def forward(self, e_p: torch.Tensor, e_r: torch.Tensor) -> torch.Tensor:\n",
        "        up = F.normalize(self.U(e_p), dim=1)        # [Bp,R]\n",
        "        vr = F.normalize(self.V(e_r), dim=1)        # [Br,R]\n",
        "        g  = torch.tanh(self.G(e_p))                # [Bp,R]\n",
        "        upg = up * (1.0 + self.gate_strength * g)\n",
        "        return upg @ vr.t() + self.bias\n",
        "\n",
        "class TwoTowerModel(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.rna = RNATower(cfg)\n",
        "        self.prot_proj = ProtMLP(cfg.PROT_EMB_DIM, cfg.PROT_MLP_HIDDEN, cfg.D_MODEL, cfg.PROT_DROPOUT)\n",
        "        self.score = GatedBilinearLowRankCosine(cfg.D_MODEL, cfg.RANK, cfg.GATE_STRENGTH)\n",
        "    def encode_rna(self, tokens: torch.Tensor, struct: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        return self.rna(tokens, struct)\n",
        "    def project_prot(self, prot_vecs: torch.Tensor) -> torch.Tensor:\n",
        "        return self.prot_proj(prot_vecs)\n",
        "\n",
        "# ==============================\n",
        "# I/O + structure + checkpoint\n",
        "# ==============================\n",
        "def load_checkpoint(ckpt_path: str, device: torch.device):\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    saved = ckpt.get(\"cfg\", {}) or {}\n",
        "\n",
        "    # Start from local defaults, then overlay only known fields\n",
        "    cfg = Config()\n",
        "    fields = set(Config.__dataclass_fields__.keys())\n",
        "    applied, skipped = [], []\n",
        "    for k, v in saved.items():\n",
        "        if k in fields:\n",
        "            setattr(cfg, k, v); applied.append(k)\n",
        "        else:\n",
        "            skipped.append(k)\n",
        "    if applied:\n",
        "        print(f\"[Cfg] Applied {len(applied)} fields from checkpoint.\")\n",
        "    if skipped:\n",
        "        print(f\"[Cfg] Skipped {len(skipped)} unknown fields (expected in standalone).\")\n",
        "\n",
        "    model = TwoTowerModel(cfg).to(device).eval()\n",
        "    ema = ckpt.get(\"ema\")\n",
        "    if isinstance(ema, dict) and \"module\" in ema:\n",
        "        model.load_state_dict(ema[\"module\"], strict=True)\n",
        "        print(\"[Load] EMA weights.\")\n",
        "    else:\n",
        "        model.load_state_dict(ckpt[\"model\"], strict=True)\n",
        "        print(\"[Load] Raw weights.\")\n",
        "    return cfg, model\n",
        "\n",
        "def load_esm_only(cfg: Config, n_rbps: int) -> torch.Tensor:\n",
        "    assert cfg.USE_ESM2 and not cfg.USE_PROTT5, \"This script expects ESM2 only.\"\n",
        "    E = np.load(ESM_TEST).astype(np.float32)\n",
        "    assert E.shape[0] == n_rbps, f\"ESM test rows ({E.shape[0]}) != RBPs ({n_rbps})\"\n",
        "    if getattr(cfg, \"PROT_SRC_ZSCORE\", False) and os.path.exists(ESM_TRAIN):\n",
        "        T = np.load(ESM_TRAIN).astype(np.float32)\n",
        "        mu, sd = T.mean(0, keepdims=True), T.std(0, keepdims=True); sd[sd < 1e-6] = 1.0\n",
        "        E = (E - mu) / sd\n",
        "        print(\"[Prot] Applied train z-score to ESM test embeddings.\")\n",
        "    print(f\"[Prot] ESM test embeddings: {E.shape}\")\n",
        "    return torch.from_numpy(E).to(DEVICE)\n",
        "\n",
        "def load_struct_slice(start_idx: int, seqs: List[str]) -> torch.Tensor:\n",
        "    \"\"\"Return [B, Lmax, 5] PHIME for RNA lines [start_idx : start_idx+B).\"\"\"\n",
        "    B = len(seqs)\n",
        "    lens = [len(s) for s in seqs]\n",
        "    Lmax = max(lens) if lens else 0\n",
        "    out = np.zeros((B, Lmax, 5), np.float32)\n",
        "    for b, i in enumerate(range(start_idx, start_idx+B)):\n",
        "        L = lens[b]\n",
        "        path = os.path.join(STRUCT_DIR, f\"seq_{i:06d}.npz\")\n",
        "        S = None\n",
        "        try:\n",
        "            with np.load(path) as z:\n",
        "                if \"PHIME\" in z:\n",
        "                    S = np.array(z[\"PHIME\"], np.float32)\n",
        "                elif \"PLUM\" in z:\n",
        "                    P, Lh, Uu, Mm = (np.array(z[\"PLUM\"], np.float32).T)\n",
        "                    I = 0.5 * Mm; M2 = Mm - I\n",
        "                    S = np.stack([P, Lh, I, M2, Uu], axis=-1).astype(np.float32)\n",
        "        except Exception:\n",
        "            S = None\n",
        "        if S is None:\n",
        "            S = np.zeros((L,5), np.float32); S[:,4] = 1.0\n",
        "        if S.shape[0] != L:\n",
        "            S = S[:L] if S.shape[0] > L else np.vstack([S, np.zeros((L - S.shape[0], 5), np.float32)])\n",
        "        out[b, :L, :] = S\n",
        "    return torch.from_numpy(out)\n",
        "\n",
        "# ==============================\n",
        "# Main (no sys.argv)\n",
        "# ==============================\n",
        "def main():\n",
        "    # Sanity checks\n",
        "    for p in [RBP_FILE, RNA_FILE, CKPT_PATH, ESM_TEST]:\n",
        "        if not os.path.exists(p):\n",
        "            raise FileNotFoundError(f\"Missing required path: {p}\")\n",
        "    if not os.path.isdir(STRUCT_DIR):\n",
        "        raise FileNotFoundError(f\"Missing struct dir: {STRUCT_DIR}\")\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "    rbps = read_lines(RBP_FILE, normalize_rna=False)\n",
        "    rnas = read_lines(RNA_FILE, normalize_rna=True)\n",
        "    print(f\"[Load] RBPs: {len(rbps)} | RNAs: {len(rnas)} | Device: {DEVICE}\")\n",
        "\n",
        "    cfg, model = load_checkpoint(CKPT_PATH, DEVICE)\n",
        "    if getattr(cfg, \"RNA_USE_STRUCT\", False) and not os.path.isdir(STRUCT_DIR):\n",
        "        raise RuntimeError(\"cfg.RNA_USE_STRUCT=True but STRUCT_DIR not found.\")\n",
        "\n",
        "    # --- Protein side: ESM only; build TTA projections once (same as training evaluate) ---\n",
        "    esm = load_esm_only(cfg, len(rbps))            # [P, Din], already z-scored if PROT_SRC_ZSCORE\n",
        "    with torch.no_grad():\n",
        "        if TTA_USE:\n",
        "            e_p_list = []\n",
        "            for k in range(TTA_N):\n",
        "                vecs = esm if k == 0 else (esm + TTA_NOISE * torch.randn_like(esm))\n",
        "                e_p_list.append(model.project_prot(vecs))  # each [P, D]\n",
        "        else:\n",
        "            e_p_single = model.project_prot(esm)           # [P, D]\n",
        "    P = (e_p_list[0] if TTA_USE else e_p_single).size(0)\n",
        "\n",
        "    # Open output files\n",
        "    files = [open(os.path.join(OUT_DIR, f\"RBP{BASE_OFFSET + i + 1}.txt\"), \"w\") for i in range(P)]\n",
        "\n",
        "    # Stream RNAs in order\n",
        "    N = len(rnas)\n",
        "    with torch.no_grad():\n",
        "        for a in range(0, N, RNA_BATCH):\n",
        "            b = min(a + RNA_BATCH, N)\n",
        "            batch = rnas[a:b]\n",
        "            tok = tokenize_rna_batch(cfg, batch).to(DEVICE)\n",
        "            st  = load_struct_slice(a, batch).to(DEVICE) if getattr(cfg, \"RNA_USE_STRUCT\", False) else None\n",
        "            e_r = model.encode_rna(tok, st)                # [B, D]\n",
        "\n",
        "            # TTA over protein side\n",
        "            if TTA_USE:\n",
        "                S_acc = None\n",
        "                for e_p in e_p_list:\n",
        "                    S_k = model.score(e_p, e_r).detach().cpu().numpy()   # [P, B]\n",
        "                    S_acc = S_k if S_acc is None else (S_acc + S_k)\n",
        "                S = S_acc / float(TTA_N)\n",
        "            else:\n",
        "                S = model.score(e_p_single, e_r).detach().cpu().numpy()  # [P, B]\n",
        "\n",
        "            # Append lines (preserve RNA order)\n",
        "            for j in range(P):\n",
        "                files[j].write(\"\".join(FMT % v + \"\\n\" for v in S[j]))\n",
        "            if ((a // RNA_BATCH) % 10) == 0:\n",
        "                print(f\"[Prog] {b}/{N} ({b/N:.1%})\")\n",
        "\n",
        "    for f in files: f.close()\n",
        "    print(f\"[Done] Wrote {P} files in {OUT_DIR}/  (RBP{BASE_OFFSET+1}.txt .. RBP{BASE_OFFSET+P}.txt)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFA2lN8MwWB2",
        "outputId": "3b9a28db-81a9-4c44-a21c-9f4a2738978b"
      },
      "outputs": [],
      "source": [
        "# Unzip struct_train.zip into ./struct_train_unzipped\n",
        "import shutil\n",
        "\n",
        "shutil.unpack_archive(\"/content/struct_test.zip\", \"struct_test\", \"zip\")\n",
        "print(\"Extracted to: struct_test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5d_H4o5ujnh"
      },
      "outputs": [],
      "source": [
        "!zip -r -q predictions.zip predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sIV8pnd4smC",
        "outputId": "92b14acd-7809-41bc-b64d-f060f641ad65"
      },
      "outputs": [],
      "source": [
        "import os, glob, zipfile\n",
        "\n",
        "src = \"predictions\"\n",
        "out = \"predictions_split9\"\n",
        "group = 9\n",
        "\n",
        "os.makedirs(out, exist_ok=True)\n",
        "\n",
        "# natural-ish sort: if you need strict natural sort, use `natsort` library; otherwise this is fine for many cases\n",
        "files = sorted(glob.glob(os.path.join(src, \"*.txt\")), key=lambda p: (os.path.basename(p).lower()))\n",
        "\n",
        "for i in range(0, len(files), group):\n",
        "    part = i//group + 1\n",
        "    zpath = os.path.join(out, f\"predictions_{part:03d}.zip\")\n",
        "    with zipfile.ZipFile(zpath, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        for f in files[i:i+group]:\n",
        "            zf.write(f, arcname=os.path.basename(f))\n",
        "    print(\"wrote\", zpath)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "371e040eb39748cfa8c134a7c1db322c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b2ba58f4374405fbb74a05d6df453f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_553bea2b980b4769874dc456b2653131",
            "placeholder": "​",
            "style": "IPY_MODEL_371e040eb39748cfa8c134a7c1db322c",
            "value": " 7/7 [05:18&lt;00:00, 37.24s/it]"
          }
        },
        "3eaadd85cede40baab72cbd2792107d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c570ea71a274386b6dbff27afaf734e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "553bea2b980b4769874dc456b2653131": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a5176bc0a4a471583cb0343af500f91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ee5697e28de4579a98f4e29cb3819af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a17605889dd542b58561ed3d429b04f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d60d22d81ca44b26b5f0a699dfe01625",
              "IPY_MODEL_b1c01b0e274f4267923ad3e38174645d",
              "IPY_MODEL_3b2ba58f4374405fbb74a05d6df453f7"
            ],
            "layout": "IPY_MODEL_3eaadd85cede40baab72cbd2792107d5"
          }
        },
        "b1c01b0e274f4267923ad3e38174645d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a5176bc0a4a471583cb0343af500f91",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ee5697e28de4579a98f4e29cb3819af",
            "value": 7
          }
        },
        "d60d22d81ca44b26b5f0a699dfe01625": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c570ea71a274386b6dbff27afaf734e",
            "placeholder": "​",
            "style": "IPY_MODEL_f81db617bf28457597351803b3ab4a92",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f81db617bf28457597351803b3ab4a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
